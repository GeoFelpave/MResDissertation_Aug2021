{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "118b17b7-e221-4e52-af0d-9eeb96eb8f6e",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "\n",
    "This notebook was created to capture the data wrangling process for all the datasets. This will include the commands used in NCO and other libraries, but it will only run code written in Python.\n",
    "\n",
    "---\n",
    "\n",
    " - Author:          \n",
    "                    Luis F Patino Velasquez - MA\n",
    " - Date:            \n",
    "                    Jun 2020\n",
    " - Version:         \n",
    "                    1.0\n",
    " - Notes:            \n",
    "                    files used in this notebook are in format netCDF\n",
    " - Jupyter version: \n",
    "                    jupyter core     : 4.7.1\n",
    "                    jupyter-notebook : 6.4.0\n",
    "                    qtconsole        : 5.1.1\n",
    "                    ipython          : 7.25.0\n",
    "                    ipykernel        : 6.0.3\n",
    "                    jupyter client   : 6.1.12\n",
    "                    jupyter lab      : 3.0.16\n",
    "                    nbconvert        : 6.1.0\n",
    "                    ipywidgets       : 7.6.3\n",
    "                    nbformat         : 5.1.3\n",
    "                    traitlets        : 5.0.5\n",
    " - Python version:  \n",
    "                    3.8.5 \n",
    "\n",
    "---\n",
    "\n",
    "## Main considerations\n",
    "\n",
    "* The main time period considered for all datasets is 2001 - 2019, this is based on GPM-IMERG data\n",
    "\n",
    "## Setting Python Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cbf02e-b735-4339-8ea1-b04840aa2944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Global vars\n",
    "sep = ('''------------\\n------------''')\n",
    "print(sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f477edd7-2e7f-4fc3-966a-d6becd615b71",
   "metadata": {},
   "source": [
    "# ERA5\n",
    "\n",
    "ERA5 data was downloaded as 24 hourly steps (0, 1, 2, 3, 4,..,23) for each calendar day starting from Jan 1 to Dec 31 of each considered year.\n",
    "\n",
    "ECMWF state here https://confluence.ecmwf.int/display/CKB/ERA5%3A+How+to+calculate+daily+total+precipitation that daily total precipitation must be calculated by accumulating precipitation for e.g. Jan 1, 1979 by summing the steps 1, 2,...,23 of Jan 1 AND step 0 of Jan 2. It means that the step 0 of Jan 1, 1979 is not included in calculation of the total precipitation for that day. For calculation of total precipitation for Jan 2, 1979 we use also the steps 1, 2, 3,...,23 of that day plus step 0 of Jan 3 and so on.\n",
    "\n",
    "* The following NCO code was used to create daily total precipitation `cdo -b F64 daysum -shifttime,-1hour era5_fileTwo.nc era5_prcp_daysum_nco_2000-2010.nc`\n",
    "* The following NCO code was used to change from m to mm `cdo mulc,1000 era5_prcp_daysum_nco_2000-2010.nc era5_prcp_daysum_mm_nco_2000-2010.nc`\n",
    "* The following NCO code was used to change the units in the tp variable `ncatted -O -a units,tp,m,c,\"mm d-1\" in.nc`\n",
    "\n",
    "*After creating the daily precipitation files the following line was used to add the standard_name to the tp variable needed for xclim*\n",
    "\n",
    "* `ncatted -O -a standard_name,tp,c,c,\"total_precipitation\" in_DAY.nc`\n",
    "\n",
    "## Creating the daily total precipitation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe7018e-70d6-40c0-8020-b1747445df42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set directory to read and for outputs\n",
    "fldr_src = Path('/mnt/d/MRes_dataset/search_data/era_copernicus_uk/')\n",
    "\n",
    "# Create list of daily total precipitation files \n",
    "annual_fls = fldr_src.glob('**/era5_copernicus_prcp_daysum_mm*')\n",
    "# annual_fls = fldr_src.glob('**/era5_prcp_daysum_nco*')\n",
    "\n",
    "# Create final file\n",
    "for era_fl in annual_fls:\n",
    "    if era_fl.name.split('_')[5][:4] != '2000':\n",
    "        yr_fst = era_fl.name.split('_')[5][:4] + '-01-01'\n",
    "        yr_lst = era_fl.name.split('_')[5][5:9] + '-12-31'\n",
    "        filename = 'era5_copernicus_DAY_prcp_' + era_fl.name.split('_')[5]\n",
    "    else:\n",
    "        yr_fst = '2001-01-01'\n",
    "        yr_lst = era_fl.name.split('_')[5][5:9] + '-12-31'\n",
    "        filename = 'era5_copernicus_DAY_prcp_' + '2001' + '-' + era_fl.name.split('_')[5][5:9] + '.nc'\n",
    "    \n",
    "    # Open data and drop the first timestep of the input data as per the ECMWF statement\n",
    "    dataset = xr.open_dataset(era_fl, decode_timedelta=False)\n",
    "    dataset_annual = dataset.sel(time=slice(yr_fst, yr_lst))\n",
    "\n",
    "    # Saving file with annual precipitations\n",
    "    annual_prcp = Path(fldr_src / filename)\n",
    "    print ('saving to ', annual_prcp)\n",
    "    dataset_annual.to_netcdf(path=annual_prcp)\n",
    "    print ('finished saving')\n",
    "    dataset.close()\n",
    "    \n",
    "print(sep)\n",
    "print('All done!! check files in {}'.format(fldr_src))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e920b0b6-d57c-487f-b509-c5d2e0c61e76",
   "metadata": {},
   "source": [
    "# HadUK-Grid\n",
    "\n",
    "HadUK-Grid data was downloaded as monthly files for each calendar day starting from Jan 1 to Dec 31 of each considered year.\n",
    "\n",
    "* The following NCO code was used to change the units in the rainfall variable `ncatted -O -a units,rainfall,m,c,\"mm d-1\" in.nc`\n",
    "\n",
    "The line of code was run as part of a the follwing bash code:\n",
    "\n",
    "```\n",
    "#! /bin/bash\n",
    "\n",
    "FILES='folder path with files/*'\n",
    "for f in $FILES\n",
    "do\n",
    "\techo \"Processing $f\"\n",
    "    ncatted -O -a units,rainfall,m,c,\"mm d-1\" $f\n",
    "done\n",
    "\n",
    "```\n",
    "\n",
    "***HadUK-Grid files did not require any data wrangling using python***\n",
    "\n",
    "## Checking File downloaded correctly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69533bf-0faf-407f-8987-85130da432a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory to read and for outputs\n",
    "fldr_src = Path('/mnt/d/MRes_dataset/search_data/haduk_cedac_uk/')\n",
    "\n",
    "# Create list with files\n",
    "fls_lst = fldr_src.glob('**/*.nc')\n",
    "\n",
    "# Check to see if the file structure is correct - ie it opens using xarray\n",
    "arr_err = []\n",
    "for fl in fls_lst:\n",
    "    try:\n",
    "        dataset = xr.open_dataset(fl, decode_timedelta=False)\n",
    "        dataset.close()\n",
    "    except:\n",
    "        arr_err.append(fl)\n",
    "        print('Doing file: {}'.format(fl))\n",
    "\n",
    "if len(arr_err) == 0:\n",
    "    print('The files were downloaded correctly')\n",
    "else:\n",
    "    print ('The following files could not be opened: {}'.format(arr_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b92c36-e8b7-451c-9904-fbd9a230780f",
   "metadata": {},
   "source": [
    "## Checking output after units change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c48b880-a0e0-4de0-88aa-d65c1bdb461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory to read and for outputs\n",
    "fldr_src = Path('/mnt/d/MRes_dataset/search_data/haduk_cedac_uk/')\n",
    "# !ls {fldr_src}\n",
    "\n",
    "dataset = xr.open_dataset(Path(fldr_src / 'rainfall_hadukgrid_uk_5km_day_20020601-20020630.nc'), decode_timedelta=False)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c5a714-faf0-4189-a957-2fd2b5b573a3",
   "metadata": {},
   "source": [
    "# GPM-IMERG\n",
    "\n",
    "ERA5 data was downloaded as daily files for each calendar day starting from Jan 1 to Dec 31 of each considered year.\n",
    "\n",
    "* The following NCO code was used to change the units in the rainfall variable `ncatted -O -a units,rainfall,m,c,\"mm d-1\" in.nc`\n",
    "\n",
    "The change of units NCO code was run as part of a the follwing bash code:\n",
    "\n",
    "```\n",
    "#! /bin/bash\n",
    "\n",
    "FILES='folder path with files/*'\n",
    "for f in $FILES\n",
    "do\n",
    "\techo \"Processing $f\"\n",
    "    ncatted -O -a units,rainfall,m,c,\"mm d-1\" $f\n",
    "done\n",
    "\n",
    "```\n",
    "\n",
    "## Checking File downloaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12709b35-c79c-4844-b5cb-560f61a23d57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set directory to read and for outputs\n",
    "fldr_src = Path('/mnt/d/MRes_dataset/search_data/gpm_imerg_nasa_uk/')\n",
    "\n",
    "# Create list with files\n",
    "fls_lst = fldr_src.glob('**/*')\n",
    "\n",
    "# Check to see if the file structure is correct - ie it opens using xarray\n",
    "arr_err = []\n",
    "for fl in fls_lst:\n",
    "    try:\n",
    "        dataset = xr.open_dataset(fl, decode_timedelta=False)\n",
    "        dataset.close()\n",
    "    except:\n",
    "        arr_err.append(fl)\n",
    "        print('Doing file: {}'.format(fl))\n",
    "\n",
    "if len(arr_err) == 0:\n",
    "    print('The files were downloaded correctly')\n",
    "else:\n",
    "    print ('The following files could not be opened: {}'.format(arr_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f47a08b-ebde-4569-901f-77e2fe7b70fb",
   "metadata": {},
   "source": [
    "## Checking output after units change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187cec6d-0223-461c-937b-73e54e654f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory to read and for outputs\n",
    "fldr_src = Path('/mnt/d/MRes_dataset/search_data/gpm_imerg_nasa_uk/')\n",
    "# !ls {fldr_src}\n",
    "\n",
    "dataset = xr.open_dataset(Path(fldr_src / '3B-DAY.MS.MRG.3IMERG.20001002-S000000-E235959.V06.nc4.nc4'), decode_timedelta=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb8d1ff-0b0b-488f-b5ce-c61e330b612e",
   "metadata": {},
   "source": [
    "# Modified HadUK-Grid from planar to geodesic coordenates\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    " <p style=\"color:black\"> <b>Caution: This should only be run after creating the indices datasets</b></p>\n",
    "</div>\n",
    "\n",
    " * This process creates a new HadUK-Grid file using lat, lon, time as the main dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c194a0a-9e2f-496f-9349-56ea12432907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "# Set directory\n",
    "folder = Path('/mnt/c/Users/C0060017/Documents/Taught_Material/MRes_Dissertation/Dissertation/MRes_dataset/active_data/102_prcp/')\n",
    "# uncomment below to check if it is the right path\n",
    "!ls {folder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa3b7ac-ffd7-4c9f-b6e1-effdb54ce056",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Original dataset\n",
    "dataset = xr.open_dataset(Path(folder / 'haduk_metoffice_xclimSeason_QSDEC_prcp_2001-2019.nc'), decode_timedelta=False)\n",
    "# dataset['prcptot'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccb0c19-1b33-4b51-a5b2-fdcf630907c2",
   "metadata": {},
   "source": [
    "## Creating the modified HadUK-Grid nc file\n",
    "\n",
    "* This process will create a nc file with the transformed coordinates. However, the output file does not get overwritten. The user will need to:\n",
    "    - Modified the variable *dsout* with a different name for the output file or,\n",
    "    - Restart the Jupyter server and re-run the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970e660a-ceb7-4877-8f12-52f02b10e860",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This produces information with regards to the CRS\n",
    "from pyproj import CRS\n",
    "crs_27700 = CRS.from_epsg(27700)\n",
    "crs_27700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80eccde-4545-4952-a933-7d09419f0a7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' This code is a modification of the code found here: https://gist.github.com/guziy/8543562'''\n",
    "import pyproj\n",
    "from pyproj import CRS\n",
    "from pyproj import Transformer, transform\n",
    "\n",
    "# Reading the data\n",
    "dsin = Dataset(Path(folder / 'haduk_metoffice_xclimSeason_QSDEC_prcp_2001-2019.nc'))\n",
    "\n",
    "#output file\n",
    "dsout = Dataset(Path(folder / 'hadukWGS84_metoffice_xclimSeason_QSDEC_prcp_2001-2019.nc'), mode='w',format='NETCDF3_CLASSIC')\n",
    "# dsout = Dataset(Path(folder / 'haduk_metoffice_xclimSeason_QSDEC_prcp_2001-2019_WGS84.nc'), mode='w',format='NETCDF3_CLASSIC')\n",
    "# dsout = Dataset(Path(folder / 'test2.nc'), mode='w',format='NETCDF4_CLASSIC')\n",
    "    \n",
    "#Create dimensions            \n",
    "for v_name, varin in dsin.variables.items():\n",
    "    if v_name in ['latitude', 'longitude', 'time', 'percentiles']:\n",
    "        if v_name == 'latitude':\n",
    "            dsout.createDimension(v_name, varin.shape[0])\n",
    "        elif v_name == 'longitude':\n",
    "            dsout.createDimension(v_name, varin.shape[1])\n",
    "        else:\n",
    "            dsout.createDimension(v_name, len(varin))\n",
    "\n",
    "##########################\n",
    "# This is for add data but r99ptot\n",
    "##########################\n",
    "\n",
    "# Copy variables\n",
    "for v_name, varin in dsin.variables.items():\n",
    "    if v_name in ['r10mm', 'r20mm', 'cdd', 'cwd', 'sdii', 'rx1day', 'rx5day', 'prcptot', 'r95ptot', 'r99ptot', 'time']:      \n",
    "        if v_name in ['r95ptot', 'r99ptot']:\n",
    "            outVar = dsout.createVariable(v_name, varin.datatype, ('time', 'latitude', 'longitude', 'percentiles'))\n",
    "            # Copy variable attributes\n",
    "            outVar.setncatts({k: varin.getncattr(k) for k in varin.ncattrs()})\n",
    "            outVar[:] = varin[:] \n",
    "        elif v_name in ['time']:\n",
    "            outVar = dsout.createVariable(v_name, np.float64, varin.dimensions)\n",
    "            # Copy variable attributes - This currently does not work for the time variable\n",
    "            outVar.setncatts({k: varin.getncattr(k) for k in varin.ncattrs()})\n",
    "            outVar[:] = varin[:] \n",
    "        else:\n",
    "            outVar = dsout.createVariable(v_name, varin.datatype, ('time', 'latitude', 'longitude'))\n",
    "            # Copy variable attributes\n",
    "            outVar.setncatts({k: varin.getncattr(k) for k in varin.ncattrs()})\n",
    "            outVar[:] = varin[:]\n",
    "\n",
    "# # # ##########################\n",
    "# # # # This is for r99ptot data\n",
    "# # # ##########################\n",
    "# # # # Copy variables\n",
    "# # # for v_name, varin in dsin.variables.items():\n",
    "# # #     if v_name in ['r99ptot', 'time']:      \n",
    "# # #         if v_name in ['r99ptot']:\n",
    "# # #             outVar = dsout.createVariable(v_name, varin.datatype, ('time', 'latitude', 'longitude', 'percentiles'))\n",
    "# # #             # Copy variable attributes\n",
    "# # #             outVar.setncatts({k: varin.getncattr(k) for k in varin.ncattrs()})\n",
    "# # #             outVar[:] = varin[:] \n",
    "# # #         else:\n",
    "# # #             outVar = dsout.createVariable(v_name, np.float64, varin.dimensions)\n",
    "# # #             # Copy variable attributes - This currently does not work for the time variable\n",
    "# # #             outVar.setncatts({k: varin.getncattr(k) for k in varin.ncattrs()})\n",
    "# # #             outVar[:] = varin[:] \n",
    "\n",
    "\n",
    "# Create coordinates variables \n",
    "y_coord = dsout.createVariable('latitude',np.float64,('latitude'))\n",
    "x_coord = dsout.createVariable('longitude',np.float64,('longitude'))\n",
    "\n",
    "\n",
    "# Read the x, y values ready to transform to WGS84\n",
    "values_lat = dsin['projection_y_coordinate'][:]\n",
    "values_lon = dsin['projection_x_coordinate'][:]\n",
    "\n",
    "\n",
    "# Information for the transformation of the coordinates from OSGB36 to WGS84\n",
    "fake_lat = np.zeros(len(values_lat)) # this will be the x when transforming y to latitudes\n",
    "fake_lon = np.zeros(len(values_lon)) # this will be the y when transforming x to longitude\n",
    "\n",
    "# # Transform coordinate to WGS84\n",
    "# Method 1\n",
    "transformer = Transformer.from_crs(\"epsg:27700\", \"epsg:4326\", always_xy=True)\n",
    "lonx_fake, laty = transformer.transform(fake_lat, np.ma.getdata(values_lat))\n",
    "lonx, laty_fake = transformer.transform(np.ma.getdata(values_lon), fake_lon)\n",
    "\n",
    "# Add transformed coordinates (WGS84) to nc file\n",
    "y_coord[:] = laty\n",
    "x_coord[:] = lonx\n",
    "\n",
    "# close the output file to\n",
    "dsout.close()\n",
    "\n",
    "# Checking output\n",
    "dataset = xr.open_dataset(Path(folder / 'hadukWGS84_metoffice_xclimSeason_QSDEC_prcp_2001-2019.nc'), decode_timedelta=False)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58df81e-7a39-41ae-bfa4-02880326d106",
   "metadata": {},
   "source": [
    "## Saving final file with attributes in coordinate variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4570ce1c-8637-42c0-983f-fb3b2f8d9b05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Encoding Example compliant with netcdf files\n",
    "\n",
    "encoding = {'lat': {'zlib': False},\n",
    "            'lon': {'zlib': False},\n",
    "            'any_variable': {'_FillValue': -999.0,\n",
    "                  'chunksizes': (1, 8, 10),\n",
    "                  'complevel': 1,\n",
    "                  'zlib': True}\n",
    "            }\n",
    "'''\n",
    "dataset = xr.open_dataset(Path(folder / 'hadukWGS84_metoffice_xclimSeason_QSDEC_prcp_2001-2019.nc'), decode_timedelta=False)\n",
    "\n",
    "dataset.latitude.attrs['units'] = 'degrees_north'\n",
    "dataset.latitude.attrs['long_name'] = 'latitude'\n",
    "dataset.latitude.attrs['origname'] = 'latitude'\n",
    "\n",
    "dataset.longitude.attrs['units'] = 'degrees_east'\n",
    "dataset.longitude.attrs['long_name'] = 'longitude'\n",
    "dataset.longitude.attrs['origname'] = 'longitude'\n",
    "\n",
    "WGS84_file = Path(folder / 'hadukWGS84Attr_metoffice_xclimSeason_QSDEC_prcp_2001-2019.nc')\n",
    "print ('saving to ', WGS84_file)\n",
    "dataset.to_netcdf(WGS84_file)\n",
    "print ('finished saving')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
