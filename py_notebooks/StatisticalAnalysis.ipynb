{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis of Extreme Precipitation Indices (EPI)\n",
    "\n",
    "This notebook was designed to carry out the statistical analysis of the output data obtained from `Computing_Indices.ipynb`. These file are the computed daily precipitation extreme indices\n",
    "\n",
    "---\n",
    "\n",
    " - Author:          \n",
    "                    Luis F Patino Velasquez - MA\n",
    " - Date:            \n",
    "                    Jun 2020\n",
    " - Version:         \n",
    "                    1.0\n",
    " - Notes:            \n",
    "                    Files used in this notebook are outputs of the Computing_Indices.ipynb notebook\n",
    " - Jupyter version: \n",
    "                    jupyter core     : 4.7.1\n",
    "                    jupyter-notebook : 6.4.0\n",
    "                    qtconsole        : 5.1.1\n",
    "                    ipython          : 7.25.0\n",
    "                    ipykernel        : 6.0.3\n",
    "                    jupyter client   : 6.1.12\n",
    "                    jupyter lab      : 3.0.16\n",
    "                    nbconvert        : 6.1.0\n",
    "                    ipywidgets       : 7.6.3\n",
    "                    nbformat         : 5.1.3\n",
    "                    traitlets        : 5.0.5\n",
    " - Python version:  \n",
    "                    3.8.5 \n",
    "\n",
    "---\n",
    "\n",
    "## Main considerations\n",
    "\n",
    "* Data coming from HadUK-Grid and GPM-IMERG have been regridded using a conservative interpolation in NCO. An example of the code used is: `cdo -remapcon,gpm_imerg_xclimSeason_QSDEC_prcp_2001-2019.nc haduk_metoffice_xclimSeason_QSDEC_prcp_2001-2019 haduk_metoffice_xclimSeason_QSDEC_prcp_RegridToIMERG_2001-2019`\n",
    "\n",
    "## Setting Python Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for xclim and xarray\n",
    "import xclim as xc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# File handling libraries\n",
    "import time\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# other python packages\n",
    "import functools\n",
    "import warnings\n",
    "from itertools import groupby\n",
    "\n",
    "# Geospatial libraries\n",
    "import geopandas\n",
    "import rioxarray\n",
    "from shapely.geometry import mapping\n",
    "\n",
    "# import plotting stuff\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.colors as mcolors\n",
    "# set colours\n",
    "# plt.style.use('default')\n",
    "plt.style.use(\"~/.local/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/lfpv.mplstyle\")\n",
    "\n",
    "%matplotlib inline\n",
    "# Set some plotting defaults\n",
    "plt.rcParams['figure.figsize'] = (15, 11)\n",
    "plt.rcParams['figure.dpi'] = 50\n",
    "\n",
    "# Mapping libraries\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "sep = '-----------\\n-----------'\n",
    "print(sep)\n",
    "\n",
    "def savingFile(file_name, data_xarray):\n",
    "    # Check if file already exist\n",
    "    check = Path(Path('/mnt/d/MRes_dataset/active_data/103_stats') / file_name)\n",
    "    if check.is_file() is False:\n",
    "        # Saving file with annual precipitations\n",
    "        xclim_indices = Path(Path('/mnt/d/MRes_dataset/active_data/103_stats') / file_name)\n",
    "        print ('saving to ', xclim_indices)\n",
    "        data_xarray.rio.set_spatial_dims(x_dim='lon', y_dim='lat', inplace=True)\n",
    "        data_xarray.rio.write_crs(4326, inplace=True)\n",
    "        data_xarray.to_netcdf(path=xclim_indices)\n",
    "        print ('finished saving')\n",
    "    else:\n",
    "        print ('{} already exist - try using a different file name'.format(file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set directory to read and for outputs\n",
    "fldr_src = Path('/mnt/d/MRes_dataset/active_data/102_prcp')\n",
    "# Check saved files\n",
    "!ls {fldr_src}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    " <p style=\"color:black\"> <b>R95pTOT and R99pTOT are run separetely. Make sure to look at column values in the script as well as the data set chosen in the step below</b></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with data excluding r99ptot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reading the seasonal data\n",
    "ERA_dataset_season = xr.open_dataset(Path(fldr_src / 'era5_copernicus_xclimSeason_QSDEC_prcp_RegridedToIMERG_2001-2019.nc'))\n",
    "GPM_dataset_season = xr.open_dataset(Path(fldr_src / 'gpm_imerg_xclimSeason_QSDEC_prcp_2001-2019.nc'))\n",
    "HAD_dataset_season = xr.open_dataset(Path(fldr_src / 'hadukWGS84Attr_metoffice_xclimSeason_QSDEC_prcp_RegridedToIMERG_2001-2019.nc'))\n",
    "\n",
    "print(sep)\n",
    "print('Dataset setup')\n",
    "print(sep)\n",
    "\n",
    "HAD_dataset_season"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking and Analysing season data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Season Analysis\n",
    "\n",
    "### Grouping data by season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Grouping by season keeping time series\n",
    "# This is needed to carry out analysis by choosing the time dimension\n",
    "# It will analysis the same season for all years\n",
    "\n",
    "# ERA5\n",
    "ERA_dataset_DJF = ERA_dataset_season.sel(time=ERA_dataset_season.time.dt.season==\"DJF\")\n",
    "ERA_dataset_MAM = ERA_dataset_season.sel(time=ERA_dataset_season.time.dt.season==\"MAM\")\n",
    "ERA_dataset_JJA = ERA_dataset_season.sel(time=ERA_dataset_season.time.dt.season==\"JJA\")\n",
    "ERA_dataset_SON = ERA_dataset_season.sel(time=ERA_dataset_season.time.dt.season==\"SON\")\n",
    "\n",
    "# IMERG\n",
    "GPM_dataset_DJF = GPM_dataset_season.sel(time=GPM_dataset_season.time.dt.season==\"DJF\")\n",
    "GPM_dataset_MAM = GPM_dataset_season.sel(time=GPM_dataset_season.time.dt.season==\"MAM\")\n",
    "GPM_dataset_JJA = GPM_dataset_season.sel(time=GPM_dataset_season.time.dt.season==\"JJA\")\n",
    "GPM_dataset_SON = GPM_dataset_season.sel(time=GPM_dataset_season.time.dt.season==\"SON\")\n",
    "\n",
    "# HADUK\n",
    "HAD_dataset_DJF = HAD_dataset_season.sel(time=HAD_dataset_season.time.dt.season==\"DJF\")\n",
    "HAD_dataset_MAM = HAD_dataset_season.sel(time=HAD_dataset_season.time.dt.season==\"MAM\")\n",
    "HAD_dataset_JJA = HAD_dataset_season.sel(time=HAD_dataset_season.time.dt.season==\"JJA\")\n",
    "HAD_dataset_SON = HAD_dataset_season.sel(time=HAD_dataset_season.time.dt.season==\"SON\")\n",
    "\n",
    "print(sep)\n",
    "print('All data have been grouped by seasons')\n",
    "print(sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistic comparison\n",
    "\n",
    "* GPM-IMERG and ERA5 data will be compared with values from HadUK-Grid\n",
    "* The analysis will be carried out by seasons for all years e.g. Pearson correlation coefficient for DJF season in 2001,2002,2003....2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats packages\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import mannwhitneyu\n",
    "from scipy.stats import variation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='func_stats'></a>\n",
    "#### Functions to be used for data processing and statistical analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def season_dataframe(HADxarray_season, GPMxarray_season, ERAxarray_season):\n",
    "    \"\"\"\n",
    "    Changes any data in timedelta64 format to float for a pandas dataframe\n",
    "    :param HADxarray_season: xarray with time variable group by season\n",
    "    :param GPMxarray_season: xarray with time variable group by season\n",
    "    :param ERAxarray_season: xarray with time variable group by season\n",
    "    :return: tupple - pandas dataframes\n",
    "    \"\"\"\n",
    "    ##############\n",
    "    # ERA5\n",
    "    ##############\n",
    "    df_era_season = ERAxarray_season.to_dataframe().reset_index()\n",
    "    # Change all timedelta64 (x days) for only the day\n",
    "    df_era_season['r10mm'] = np.where(np.isnat(df_era_season['r10mm']),np.nan, (df_era_season['r10mm'] / np.timedelta64(1, 'D'))).astype(float)\n",
    "    df_era_season['r20mm'] = np.where(np.isnat(df_era_season['r20mm']),np.nan, (df_era_season['r20mm'] / np.timedelta64(1, 'D'))).astype(float)\n",
    "    df_era_season['cdd'] = np.where(np.isnat(df_era_season['cdd']),np.nan, (df_era_season['cdd'] / np.timedelta64(1, 'D'))).astype(float)\n",
    "    df_era_season['cwd'] = np.where(np.isnat(df_era_season['cwd']),np.nan, (df_era_season['cwd'] / np.timedelta64(1, 'D'))).astype(float)\n",
    "    ##############\n",
    "    # GPM-IMERG\n",
    "    ##############\n",
    "    df_gpm_season = GPMxarray_season.to_dataframe().reset_index()\n",
    "    # Change all timedelta64 (x days) for only the day\n",
    "    df_gpm_season['r10mm'] = np.where(np.isnat(df_gpm_season['r10mm']),np.nan, (df_gpm_season['r10mm'] / np.timedelta64(1, 'D'))).astype(float)\n",
    "    df_gpm_season['r20mm'] = np.where(np.isnat(df_gpm_season['r20mm']),np.nan, (df_gpm_season['r20mm'] / np.timedelta64(1, 'D'))).astype(float)\n",
    "    df_gpm_season['cdd'] = np.where(np.isnat(df_gpm_season['cdd']),np.nan, (df_gpm_season['cdd'] / np.timedelta64(1, 'D'))).astype(float)\n",
    "    df_gpm_season['cwd'] = np.where(np.isnat(df_gpm_season['cwd']),np.nan, (df_gpm_season['cwd'] / np.timedelta64(1, 'D'))).astype(float)\n",
    "    \n",
    "    ##############\n",
    "    # HADUK-GRID\n",
    "    ##############\n",
    "    df_had_season = HADxarray_season.to_dataframe().reset_index()\n",
    "    # Change all timedelta64 (x days) for only the day\n",
    "    # If value is nan then leave it as nan otherwise change to date number as a float\n",
    "    df_had_season['r10mm'] = np.where(np.isnat(df_had_season['r10mm']),np.nan, (df_had_season['r10mm'] / np.timedelta64(1, 'D')))\n",
    "    df_had_season['r20mm'] = np.where(np.isnat(df_had_season['r20mm']),np.nan, (df_had_season['r20mm'] / np.timedelta64(1, 'D')))\n",
    "    df_had_season['cdd'] = np.where(np.isnat(df_had_season['cdd']),np.nan, (df_had_season['cdd'] / np.timedelta64(1, 'D')))\n",
    "    df_had_season['cwd'] = np.where(np.isnat(df_had_season['cwd']),np.nan, (df_had_season['cwd'] / np.timedelta64(1, 'D')))\n",
    "    \n",
    "    return(df_era_season, df_gpm_season, df_had_season)\n",
    "\n",
    "def setting_dataframes(OBS_dataframe, OTH_dataframe):\n",
    "    \"\"\"\n",
    "    Creates dataframe with data for the UK only\n",
    "    :OBS_dataframe: pandas dataframe\n",
    "    :df_had_season: pandas dataframe\n",
    "    :return: tupple - pandas dataframes\n",
    "    \"\"\"\n",
    "    col_lst = ['r10mm_x', 'r20mm_x', 'cdd_x', 'cwd_x', 'sdii_x', 'rx1day_x', 'rx5day_x', 'prcptot_x', 'r95ptot_x']\n",
    "    col_lst_ord = ['lat', 'lon', 'r10mm', 'r20mm', 'cdd', 'cwd', 'sdii', 'rx1day', 'rx5day', 'prcptot', 'r95ptot']\n",
    "\n",
    "    # Add date as string to help with inner join\n",
    "    df_OBS = OBS_dataframe.copy()\n",
    "    df_OBS['tiempo'] = OBS_dataframe['time'].apply(lambda x: x.strftime('%Y%m%d'))\n",
    "    df_OTH = OTH_dataframe.copy()\n",
    "    df_OTH['tiempo'] = OTH_dataframe['time'].apply(lambda x: x.strftime('%Y%m%d'))\n",
    "\n",
    "    # Inner join of dataframe - make sure we are looking at the same set of coordinates\n",
    "    new_df = pd.merge(df_OBS, df_OTH, how='inner', on=['lat','lon','tiempo'])\n",
    "\n",
    "    # Select rows without NaN values - Only looking at the indices of the obs dataset (HadUK-Grid)\n",
    "    # Needed for the statistic analysis. It only remove row if all NaN values\n",
    "    selected_rows = new_df.dropna(subset=col_lst, how='all', axis=0).reset_index()\n",
    "\n",
    "\n",
    "    # Split the joined dataframe into OBS (HAD) and OTH (GPM or ERA)\n",
    "    df_OBS_split = selected_rows.filter(regex='_x')\n",
    "    df_OTH_split = selected_rows.filter(regex='_y')\n",
    "\n",
    "    # create copy of dataframe to avoid the slice error\n",
    "    df_OBS_final = df_OBS_split.copy()\n",
    "    df_OTH_final = df_OTH_split.copy()\n",
    "\n",
    "    # add lat and lon to split dataframes\n",
    "    coord_lst = ['lat', 'lon']\n",
    "    for coord in coord_lst:\n",
    "        df_OBS_final[coord] = selected_rows[coord]\n",
    "        df_OTH_final[coord] = selected_rows[coord]\n",
    "\n",
    "    # rename and reorder columns\n",
    "    df_OBS_final.columns = df_OBS_final.columns.str.replace('_x','')\n",
    "    df_OTH_final.columns = df_OTH_final.columns.str.replace('_y','')\n",
    "    # re-order\n",
    "    df_OBS_final = df_OBS_final.reindex(columns=col_lst_ord)\n",
    "    df_OTH_final = df_OTH_final.reindex(columns=col_lst_ord)\n",
    "\n",
    "    df_OBS_grouped = df_OBS_final.groupby(['lat', 'lon']).agg(lambda x: list(x)).reset_index()\n",
    "    df_OTH_grouped = df_OTH_final.groupby(['lat', 'lon']).agg(lambda x: list(x)).reset_index()\n",
    "    \n",
    "    return(df_OBS_grouped,df_OTH_grouped)\n",
    "\n",
    "def stats_indices_v2(dataframe_obs_season, dataframe_oth_season, stat_method):\n",
    "    \"\"\"\n",
    "    Returns xarray containing the output of statistical analysis\n",
    "    :dataframe_obs_season: pandas dataframe\n",
    "    :dataframe_oth_season: pandas dataframe\n",
    "    :stat_method: string of statistical test\n",
    "    :return: xarray\n",
    "    \"\"\"\n",
    "    col_lst = ['r10mm', 'r20mm', 'cdd', 'cwd', 'sdii', 'rx1day', 'rx5day', 'prcptot', 'r95ptot']\n",
    "    \n",
    "    # Correct the coordinaes missmatch\n",
    "    matchCoord_dfs = setting_dataframes(dataframe_obs_season, dataframe_oth_season)\n",
    "    \n",
    "    # Set the dataset taking output from function\n",
    "    groupsOBS = matchCoord_dfs[0]   \n",
    "    groupsOTH = matchCoord_dfs[1]\n",
    "    \n",
    "    ############################################################\n",
    "    # Stats are based on OTHER compared to OBSERVATION\n",
    "    # OBSERVATION = HadUK-Grid / OTHER = GPM-IMERG OR ERA5\n",
    "    ############################################################\n",
    "    # Set values ready for iteration inside dataframe\n",
    "    df_OBS_rows = groupsOBS.shape[0]\n",
    "    df_OBS_cols = groupsOBS.shape[1]\n",
    "\n",
    "    # Create df to store results from pearsons\n",
    "    df_stats = pd.DataFrame(columns=col_lst)\n",
    "\n",
    "    # Loop through dataframes getting list of values ready for statistic evaluation\n",
    "    for i in range(0,df_OBS_rows):\n",
    "        temp_lst = []\n",
    "        for j in range(2,df_OBS_cols): # indices start from column 3 in the dataframe        \n",
    "            # set list of index to be removed\n",
    "            OBS_toRemove_idx = []\n",
    "            OTH_toRemove_idx = []\n",
    "            # check if nan value exist and get index\n",
    "            obs_data = np.array(groupsOBS.iat[i,j])\n",
    "            oth_data = np.array(groupsOTH.iat[i,j])\n",
    "            OBS_toRemove = np.argwhere(np.isnan(obs_data))\n",
    "            OTH_toRemove = np.argwhere(np.isnan(oth_data))\n",
    "            # Calculate stats\n",
    "            # check if more than half of the list are NaN values inside the list\n",
    "            if (len(OBS_toRemove) < 5) or (len(OTH_toRemove) < 5): # add NaN if over half of the data is NaN values\n",
    "                # replace NaN values with the average of the list ignoring NaN values\n",
    "                obs_data[np.isnan(obs_data)] = np.nanmean(obs_data)\n",
    "                oth_data[np.isnan(oth_data)] = np.nanmean(oth_data)\n",
    "        \n",
    "                # calculate the requiered stats\n",
    "                if stat_method == 'pearson':\n",
    "                    pearson = stats.pearsonr(obs_data, oth_data)\n",
    "                    temp_lst.append(pearson[0])\n",
    "                if stat_method == 'spearman':\n",
    "                    spearm = stats.spearmanr(obs_data, oth_data)\n",
    "                    temp_lst.append(spearm[0])\n",
    "                if stat_method == 'wm':\n",
    "                    wm = stats.mannwhitneyu(obs_data, oth_data)\n",
    "                    temp_lst.append(wm[1])\n",
    "                if stat_method == 'levene':\n",
    "                    levene = stats.levene(obs_data, oth_data, center='median')\n",
    "                    temp_lst.append(levene[1])\n",
    "            else:\n",
    "                # As NaN values are too many then correlation cannot be calculated - NaN assigned\n",
    "                temp_lst.append(np.nan)\n",
    "        # add data to final dataframe\n",
    "        df_stats.loc[len(df_stats)] = temp_lst\n",
    "\n",
    "    # Add coordinate, time and percentiles variables back ready for xarray\n",
    "    df_stats['lat'] = groupsOBS['lat']\n",
    "    df_stats['lon'] = groupsOBS['lon']\n",
    "\n",
    "    # Reorder the columns and reset index\n",
    "    df_statsFinal = df_stats.reindex(columns= ['lat', 'lon', 'r10mm', 'r20mm', 'cdd', 'cwd', 'sdii', 'rx1day', 'rx5day', 'prcptot', 'r95ptot'])\n",
    "    df_statsFinal = df_statsFinal.reset_index()\n",
    "    df_statsFinal = df_statsFinal.drop(['index'], axis = 1)\n",
    "\n",
    "    # Return pearson dataframe to an xarray object\n",
    "    final_output = df_statsFinal.set_index(['lat', 'lon']).to_xarray()\n",
    "    \n",
    "    return (final_output)\n",
    "\n",
    "def reference_dataframes(OBS_dataframe, OTH_dataframe):\n",
    "    \"\"\"\n",
    "    Creates dataframe with data for the UK only\n",
    "    :OBS_dataframe: pandas dataframe\n",
    "    :df_had_season: pandas dataframe\n",
    "    :return: pandas dataframes\n",
    "    \"\"\"\n",
    "    col_lst = ['r10mm_x', 'r20mm_x', 'cdd_x', 'cwd_x', 'sdii_x', 'rx1day_x', 'rx5day_x', 'prcptot_x', 'r95ptot_x']\n",
    "    col_lst_ord = ['lat', 'lon', 'r10mm', 'r20mm', 'cdd', 'cwd', 'sdii', 'rx1day', 'rx5day', 'prcptot', 'r95ptot']\n",
    "\n",
    "    # Add date as string to help with inner join df_HAD_DJF,df_HAD_DJF\n",
    "    df_OBS = OBS_dataframe.copy()\n",
    "    df_OBS['tiempo'] = OBS_dataframe['time'].apply(lambda x: x.strftime('%Y%m%d'))\n",
    "    df_OTH = OTH_dataframe.copy()\n",
    "    df_OTH['tiempo'] = OTH_dataframe['time'].apply(lambda x: x.strftime('%Y%m%d'))\n",
    "\n",
    "    # Inner join of dataframe - make sure we are looking at the same set of coordinates\n",
    "    new_df = pd.merge(df_OBS, df_OTH, how='inner', on=['lat','lon','tiempo'])\n",
    "\n",
    "    # Select rows without NaN values for obs dataset (HadUK-Grid) - This make sure we are looking at UK extent\n",
    "    selected_rows = new_df.dropna(subset=col_lst, how='all', axis=0).reset_index()\n",
    "\n",
    "    # Split the joined dataframe into OBS (HAD) and OTH (GPM or ERA)\n",
    "    df_OTH_split = selected_rows.filter(regex='_y')\n",
    "\n",
    "    # create copy of dataframe to avoid the slice error\n",
    "    df_OTH_final = df_OTH_split.copy()\n",
    "\n",
    "    # add lat and lon to split dataframes\n",
    "    coord_lst = ['lat', 'lon']\n",
    "    for coord in coord_lst:\n",
    "        df_OTH_final[coord] = selected_rows[coord]\n",
    "\n",
    "    # rename and reorder columns\n",
    "    df_OTH_final.columns = df_OTH_final.columns.str.replace('_y','')\n",
    "    # re-order\n",
    "    df_OTH_final = df_OTH_final.reindex(columns=col_lst_ord)\n",
    "\n",
    "    df_OTH_grouped = df_OTH_final.groupby(['lat', 'lon']).agg(lambda x: list(x)).reset_index()\n",
    "\n",
    "    return(df_OTH_grouped)\n",
    "\n",
    "def indices_avg(OBS_dataframe, OTH_dataframe):\n",
    "    \"\"\"\n",
    "    Returns xarray containing the output of the average analysis\n",
    "    :OBS_dataframe: pandas dataframe\n",
    "    :OTH_dataframe: pandas dataframe\n",
    "    :return: xarray\n",
    "    \"\"\"\n",
    "    col_lst = ['r10mm', 'r20mm', 'cdd', 'cwd', 'sdii', 'rx1day', 'rx5day', 'prcptot', 'r95ptot']\n",
    "    # Correct the coordinaes missmatch\n",
    "    matchCoord_dfs = reference_dataframes(OBS_dataframe, OTH_dataframe)\n",
    "    \n",
    "    # Set the dataset taking output from function\n",
    "    groupsOTH = matchCoord_dfs\n",
    "\n",
    "    # Set values ready for iteration inside dataframe\n",
    "    df_OTH_rows = groupsOTH.shape[0]\n",
    "    df_OTH_cols = groupsOTH.shape[1]\n",
    "\n",
    "    # Create df to store results from pearsons\n",
    "    df_stats = pd.DataFrame(columns= col_lst)\n",
    "\n",
    "    # Loop through dataframes getting list of values ready for statistic evaluation\n",
    "    for i in range(0,df_OTH_rows):\n",
    "        temp_lst = []\n",
    "        for j in range(2,df_OTH_cols):  # indices start from column 3 in the dataframe  \n",
    "            # set list of index to be removed\n",
    "            OTH_toRemove_idx = []\n",
    "            # check if nan value exist and get index\n",
    "            OTH_toRemove = np.argwhere(np.isnan(groupsOTH.iat[i,j]))\n",
    "\n",
    "            # Calculate stats\n",
    "            # check if all NaN values inside the list\n",
    "            if len(OTH_toRemove) > 10: # add NaN if over half of the data is NaN values\n",
    "                temp_lst.append(np.nan)\n",
    "            else:\n",
    "                # create dataset for stats analysis\n",
    "                OTH_data = groupsOTH.iat[i,j]\n",
    "                temp_lst.append(np.nanmean(OTH_data))\n",
    "        # add data to final dataframe\n",
    "        df_stats.loc[len(df_stats)] = temp_lst\n",
    "    # Add coordinate, time and percentiles variables back ready for xarray\n",
    "    df_stats['lat'] = groupsOTH['lat']\n",
    "    df_stats['lon'] = groupsOTH['lon']\n",
    "\n",
    "    # Reorder the columns and reset index\n",
    "    df_statsFinal = df_stats.reindex(columns= ['lat', 'lon', 'r10mm', 'r20mm', 'cdd', 'cwd', 'sdii', 'rx1day', 'rx5day', 'prcptot', 'r95ptot'])\n",
    "    df_statsFinal = df_statsFinal.reset_index()\n",
    "    df_statsFinal = df_statsFinal.drop(['index'], axis = 1)\n",
    "\n",
    "    # Return pearson dataframe to an xarray object\n",
    "    final_output = df_statsFinal.set_index(['lat', 'lon']).to_xarray()\n",
    "\n",
    "    return (final_output)\n",
    "\n",
    "print(sep)\n",
    "print('Functions set')\n",
    "print(sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Change xarray data to daframe to carryout statistic analysis using pandas as ref**\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    " <p style=\"color:black\"> <b>NOTE: GPM-IMERG AND ERA5 need to be compared against is HADUK-Grid</b></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the season dataset containing HADUK-Grid, ERA5 and GPM-IMERG data \n",
    "#  the order of the function return is (df_era_season, df_gpm_season, df_had_season)\n",
    "df_DJF_season = season_dataframe(HAD_dataset_DJF, GPM_dataset_DJF, ERA_dataset_DJF)\n",
    "df_MAM_season = season_dataframe(HAD_dataset_MAM, GPM_dataset_MAM, ERA_dataset_MAM)\n",
    "df_JJA_season = season_dataframe(HAD_dataset_JJA, GPM_dataset_JJA, ERA_dataset_JJA)\n",
    "df_SON_season = season_dataframe(HAD_dataset_SON, GPM_dataset_SON, ERA_dataset_SON)\n",
    "\n",
    "# DJF Season ONLY dataframe\n",
    "df_ERA_DJF = df_DJF_season[0]\n",
    "df_GPM_DJF = df_DJF_season[1]\n",
    "df_HAD_DJF = df_DJF_season[2]\n",
    "\n",
    "# MAM Season ONLY dataframe\n",
    "df_ERA_MAM = df_MAM_season[0]\n",
    "df_GPM_MAM = df_MAM_season[1]\n",
    "df_HAD_MAM = df_MAM_season[2]\n",
    "\n",
    "# JJA Season ONLY dataframe\n",
    "df_ERA_JJA = df_JJA_season[0]\n",
    "df_GPM_JJA = df_JJA_season[1]\n",
    "df_HAD_JJA = df_JJA_season[2]\n",
    "\n",
    "# SON Season ONLY dataframe\n",
    "df_ERA_SON = df_SON_season[0]\n",
    "df_GPM_SON = df_SON_season[1]\n",
    "df_HAD_SON = df_SON_season[2]\n",
    "\n",
    "print(sep)\n",
    "print('All season datasets have been set')\n",
    "print(sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    " <p style=\"color:black\"> <b>Below is a quick check for NaN values. We are checking that the dataframes produced from the computed indices nc files are not storing only NaN values</b><br><i>This only checks if a whole row has NaN values. Further down the process more NaN value check need to be made</i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checking if there is a dataframe with all values as NaN\n",
    "vars = [\"df_ERA_DJF\", \"df_GPM_DJF\", \"df_HAD_DJF\",\n",
    "        \"df_ERA_MAM\", \"df_GPM_MAM\", \"df_HAD_MAM\",\n",
    "        \"df_ERA_JJA\", \"df_GPM_JJA\", \"df_HAD_JJA\",\n",
    "        \"df_ERA_SON\", \"df_GPM_SON\", \"df_HAD_SON\"]\n",
    "\n",
    "i = 0\n",
    "dfNames_withOnlyNan = []\n",
    "# This loop uses the name in the vars list and calls the local variable\n",
    "for name in vars:\n",
    "    selected_rows = locals()[name][~locals()[name].isnull().any(axis=1)]\n",
    "    if selected_rows.shape == 0:\n",
    "        dfNames_withOnlyNan.append(name)\n",
    "if i == 0:\n",
    "    print(sep)\n",
    "    print('All dataframes have numeric values that can be computed')\n",
    "    print(sep)\n",
    "else:\n",
    "    print(sep)\n",
    "    print('The following dataframes are entiry build with NaN values: '.format(dfNames_withOnlyNan))\n",
    "    print(sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Pearson correlation coefficient (r)\n",
    "The function can be seen [here](#func_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculation (r) for each season\n",
    "print(sep)\n",
    "print('Calculating Pearson correlation...')\n",
    "print(sep)\n",
    "\n",
    "# DJF Season ONLY dataframe\n",
    "print('Doing DJF...')\n",
    "pearson_HAD_GPM_DJF = stats_indices_v2(df_HAD_DJF, df_GPM_DJF, 'pearson')\n",
    "pearson_HAD_ERA_DJF = stats_indices_v2(df_HAD_DJF, df_ERA_DJF, 'pearson')\n",
    "print('Doing MAM...')\n",
    "# MAM Season ONLY dataframe\n",
    "pearson_HAD_GPM_MAM = stats_indices_v2(df_HAD_MAM, df_GPM_MAM, 'pearson')\n",
    "pearson_HAD_ERA_MAM = stats_indices_v2(df_HAD_MAM, df_ERA_MAM, 'pearson')\n",
    "print('Doing JJA...')\n",
    "# JJA Season ONLY dataframe\n",
    "pearson_HAD_GPM_JJA = stats_indices_v2(df_HAD_JJA, df_GPM_JJA, 'pearson')\n",
    "pearson_HAD_ERA_JJA = stats_indices_v2(df_HAD_JJA, df_ERA_JJA, 'pearson')\n",
    "print('Doing SON...')\n",
    "# SON Season ONLY dataframe\n",
    "pearson_HAD_GPM_SON = stats_indices_v2(df_HAD_SON, df_GPM_SON, 'pearson')\n",
    "pearson_HAD_ERA_SON = stats_indices_v2(df_HAD_SON, df_ERA_SON, 'pearson')\n",
    "\n",
    "print('Saving outputs of correlation as .nc files')\n",
    "savingFile('pearson_HAD_GPM_DJF.nc', pearson_HAD_GPM_DJF)\n",
    "savingFile('pearson_HAD_ERA_DJF.nc', pearson_HAD_ERA_DJF)\n",
    "savingFile('pearson_HAD_GPM_MAM.nc', pearson_HAD_GPM_MAM)\n",
    "savingFile('pearson_HAD_ERA_MAM.nc', pearson_HAD_ERA_MAM)\n",
    "savingFile('pearson_HAD_GPM_JJA.nc', pearson_HAD_GPM_JJA)\n",
    "savingFile('pearson_HAD_ERA_JJA.nc', pearson_HAD_ERA_JJA)\n",
    "savingFile('pearson_HAD_GPM_SON.nc', pearson_HAD_GPM_SON)\n",
    "savingFile('pearson_HAD_ERA_SON.nc', pearson_HAD_ERA_SON)\n",
    "\n",
    "\n",
    "print(sep)\n",
    "print('Finished calculating Pearson correlations...')\n",
    "print(sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spearman correlation coefficient (r)\n",
    "The function can be seen [here](#func_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculation (r) for each season\n",
    "print(sep)\n",
    "print('Calculating Spearman correlation...')\n",
    "print(sep)\n",
    "print('Doing DJF...')\n",
    "# DJF Season ONLY dataframe\n",
    "spearman_HAD_GPM_DJF = stats_indices_v2(df_HAD_DJF, df_GPM_DJF, 'spearman')\n",
    "spearman_HAD_ERA_DJF = stats_indices_v2(df_HAD_DJF, df_ERA_DJF, 'spearman')\n",
    "print('Doing MAM...')\n",
    "# MAM Season ONLY dataframe\n",
    "spearman_HAD_GPM_MAM = stats_indices_v2(df_HAD_MAM, df_GPM_MAM, 'spearman')\n",
    "spearman_HAD_ERA_MAM = stats_indices_v2(df_HAD_MAM, df_ERA_MAM, 'spearman')\n",
    "print('Doing JJA...')\n",
    "# JJA Season ONLY dataframe\n",
    "spearman_HAD_GPM_JJA = stats_indices_v2(df_HAD_JJA, df_GPM_JJA, 'spearman')\n",
    "spearman_HAD_ERA_JJA = stats_indices_v2(df_HAD_JJA, df_ERA_JJA, 'spearman')\n",
    "print('Doing SON...')\n",
    "# SON Season ONLY dataframe\n",
    "spearman_HAD_GPM_SON = stats_indices_v2(df_HAD_SON, df_GPM_SON, 'spearman')\n",
    "spearman_HAD_ERA_SON = stats_indices_v2(df_HAD_SON, df_ERA_SON, 'spearman')\n",
    "\n",
    "print('Saving outputs of spearman correlation as .nc files')\n",
    "savingFile('spearman_HAD_GPM_DJF.nc', spearman_HAD_GPM_DJF)\n",
    "savingFile('spearman_HAD_ERA_DJF.nc', spearman_HAD_ERA_DJF)\n",
    "savingFile('spearman_HAD_GPM_MAM.nc', spearman_HAD_GPM_MAM)\n",
    "savingFile('spearman_HAD_ERA_MAM.nc', spearman_HAD_ERA_MAM)\n",
    "savingFile('spearman_HAD_GPM_JJA.nc', spearman_HAD_GPM_JJA)\n",
    "savingFile('spearman_HAD_ERA_JJA.nc', spearman_HAD_ERA_JJA)\n",
    "savingFile('spearman_HAD_GPM_SON.nc', spearman_HAD_GPM_SON)\n",
    "savingFile('spearman_HAD_ERA_SON.nc', spearman_HAD_ERA_SON)\n",
    "\n",
    "print(sep)\n",
    "print('Finished calculating Spearman correlations...')\n",
    "print(sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mann-Whitney U test\n",
    "The function can be seen [here](#func_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculation (p) for each season\n",
    "print(sep)\n",
    "print('Calculating Mann-Whitney...')\n",
    "print(sep)\n",
    "print('Doing DJF...')\n",
    "# DJF Season ONLY dataframe\n",
    "MW_HAD_GPM_DJF = stats_indices_v2(df_HAD_DJF, df_GPM_DJF, 'wm')\n",
    "MW_HAD_ERA_DJF = stats_indices_v2(df_HAD_DJF, df_ERA_DJF, 'wm')\n",
    "print('Doing MAM...')\n",
    "# MAM Season ONLY dataframe\n",
    "MW_HAD_GPM_MAM = stats_indices_v2(df_HAD_MAM, df_GPM_MAM, 'wm')\n",
    "MW_HAD_ERA_MAM = stats_indices_v2(df_HAD_MAM, df_ERA_MAM, 'wm')\n",
    "print('Doing JJA...')\n",
    "# JJA Season ONLY dataframe\n",
    "MW_HAD_GPM_JJA = stats_indices_v2(df_HAD_JJA, df_GPM_JJA, 'wm')\n",
    "MW_HAD_ERA_JJA = stats_indices_v2(df_HAD_JJA, df_ERA_JJA, 'wm')\n",
    "print('Doing SON...')\n",
    "# SON Season ONLY dataframe\n",
    "MW_HAD_GPM_SON = stats_indices_v2(df_HAD_SON, df_GPM_SON, 'wm')\n",
    "MW_HAD_ERA_SON = stats_indices_v2(df_HAD_SON, df_ERA_SON, 'wm')\n",
    "\n",
    "print('Saving outputs of Mann-Whitney  as .nc files')\n",
    "savingFile('MW_HAD_GPM_DJF.nc', MW_HAD_GPM_DJF)\n",
    "savingFile('MW_HAD_ERA_DJF.nc', MW_HAD_ERA_DJF)\n",
    "savingFile('MW_HAD_GPM_MAM.nc', MW_HAD_GPM_MAM)\n",
    "savingFile('MW_HAD_ERA_MAM.nc', MW_HAD_ERA_MAM)\n",
    "savingFile('MW_HAD_GPM_JJA.nc', MW_HAD_GPM_JJA)\n",
    "savingFile('MW_HAD_ERA_JJA.nc', MW_HAD_ERA_JJA)\n",
    "savingFile('MW_HAD_GPM_SON.nc', MW_HAD_GPM_SON)\n",
    "savingFile('MW_HAD_ERA_SON.nc', MW_HAD_ERA_SON)\n",
    "\n",
    "print(sep)\n",
    "print('Finished calculating Mann-Whitney...')\n",
    "print(sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Levene test based on the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculation (p) for each season\n",
    "print(sep)\n",
    "print('Calculating Levene...')\n",
    "print(sep)\n",
    "print('Doing DJF...')\n",
    "# DJF Season ONLY dataframe\n",
    "levene_HAD_GPM_DJF = stats_indices_v2(df_HAD_DJF, df_GPM_DJF, 'levene')\n",
    "levene_HAD_ERA_DJF = stats_indices_v2(df_HAD_DJF, df_ERA_DJF, 'levene')\n",
    "print('Doing MAM...')\n",
    "# MAM Season ONLY dataframe\n",
    "levene_HAD_GPM_MAM = stats_indices_v2(df_HAD_MAM, df_GPM_MAM, 'levene')\n",
    "levene_HAD_ERA_MAM = stats_indices_v2(df_HAD_MAM, df_ERA_MAM, 'levene')\n",
    "print('Doing JJA...')\n",
    "# JJA Season ONLY dataframe\n",
    "levene_HAD_GPM_JJA = stats_indices_v2(df_HAD_JJA, df_GPM_JJA, 'levene')\n",
    "levene_HAD_ERA_JJA = stats_indices_v2(df_HAD_JJA, df_ERA_JJA, 'levene')\n",
    "print('Doing SON...')\n",
    "# SON Season ONLY dataframe\n",
    "levene_HAD_GPM_SON = stats_indices_v2(df_HAD_SON, df_GPM_SON, 'levene')\n",
    "levene_HAD_ERA_SON = stats_indices_v2(df_HAD_SON, df_ERA_SON, 'levene')\n",
    "\n",
    "print('Saving outputs of Levene as .nc files')\n",
    "savingFile('levene_HAD_GPM_DJF.nc', levene_HAD_GPM_DJF)\n",
    "savingFile('levene_HAD_ERA_DJF.nc', levene_HAD_ERA_DJF)\n",
    "savingFile('levene_HAD_GPM_MAM.nc', levene_HAD_GPM_MAM)\n",
    "savingFile('levene_HAD_ERA_MAM.nc', levene_HAD_ERA_MAM)\n",
    "savingFile('levene_HAD_GPM_JJA.nc', levene_HAD_GPM_JJA)\n",
    "savingFile('levene_HAD_ERA_JJA.nc', levene_HAD_ERA_JJA)\n",
    "savingFile('levene_HAD_GPM_SON.nc', levene_HAD_GPM_SON)\n",
    "savingFile('levene_HAD_ERA_SON.nc', levene_HAD_ERA_SON)\n",
    "\n",
    "print(sep)\n",
    "print('Finished calculating Levene...')\n",
    "print(sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precipitation indices average for 2001 to 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculation indices average for the entire time period\n",
    "print(sep)\n",
    "print('Calculating indices average...')\n",
    "print(sep)\n",
    "print('Doing DJF...')\n",
    "# DJF Season ONLY dataframe indices_avg(OBS_dataframe, OTH_dataframe)\n",
    "IndAvg_HAD_DJF = indices_avg(df_HAD_DJF,df_HAD_DJF)\n",
    "IndAvg_GPM_DJF = indices_avg(df_HAD_DJF,df_GPM_DJF) \n",
    "IndAvg_ERA_DJF = indices_avg(df_HAD_DJF,df_ERA_DJF) \n",
    "print('Doing MAM...')\n",
    "# MAM Season ONLY dataframe\n",
    "IndAvg_HAD_MAM = indices_avg(df_HAD_MAM,df_HAD_MAM)\n",
    "IndAvg_GPM_MAM = indices_avg(df_HAD_MAM,df_GPM_MAM) \n",
    "IndAvg_ERA_MAM = indices_avg(df_HAD_MAM,df_ERA_MAM) \n",
    "print('Doing JJA...')\n",
    "# JJA Season ONLY dataframe\n",
    "IndAvg_HAD_JJA = indices_avg(df_HAD_JJA,df_HAD_JJA)\n",
    "IndAvg_GPM_JJA = indices_avg(df_HAD_JJA,df_GPM_JJA) \n",
    "IndAvg_ERA_JJA = indices_avg(df_HAD_JJA,df_ERA_JJA) \n",
    "print('Doing SON...')\n",
    "# SON Season ONLY dataframe\n",
    "IndAvg_HAD_SON = indices_avg(df_HAD_SON,df_HAD_SON)\n",
    "IndAvg_GPM_SON = indices_avg(df_HAD_SON,df_GPM_SON) \n",
    "IndAvg_ERA_SON = indices_avg(df_HAD_SON,df_ERA_SON) \n",
    "\n",
    "print('Saving outputs of spearman correlation as .nc files')\n",
    "savingFile('IndAvg_HAD_DJF.nc', IndAvg_HAD_DJF)\n",
    "savingFile('IndAvg_GPM_DJF.nc', IndAvg_GPM_DJF)\n",
    "savingFile('IndAvg_ERA_DJF.nc', IndAvg_ERA_DJF)\n",
    "savingFile('IndAvg_HAD_MAM.nc', IndAvg_HAD_MAM)\n",
    "savingFile('IndAvg_GPM_MAM.nc', IndAvg_GPM_MAM)\n",
    "savingFile('IndAvg_ERA_MAM.nc', IndAvg_ERA_MAM)\n",
    "savingFile('IndAvg_HAD_JJA.nc', IndAvg_HAD_JJA)\n",
    "savingFile('IndAvg_GPM_JJA.nc', IndAvg_GPM_JJA)\n",
    "savingFile('IndAvg_ERA_JJA.nc', IndAvg_ERA_JJA)\n",
    "savingFile('IndAvg_HAD_SON.nc', IndAvg_HAD_SON)\n",
    "savingFile('IndAvg_GPM_SON.nc', IndAvg_GPM_SON)\n",
    "savingFile('IndAvg_ERA_SON.nc', IndAvg_ERA_SON)\n",
    "\n",
    "\n",
    "print(sep)\n",
    "print('Finished calculating indices average...')\n",
    "print(sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating graphs and Maps\n",
    "\n",
    "* Colour functions Source\n",
    "        ----------\n",
    "        https://towardsdatascience.com/beautiful-custom-colormaps-with-matplotlib-5bab3d1f0e72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex_to_rgb(value):\n",
    "    '''\n",
    "    Converts hex to rgb colours\n",
    "    value: string of 6 characters representing a hex colour.\n",
    "    Returns: list length 3 of RGB values'''\n",
    "    value = value.strip(\"#\") # removes hash symbol if present\n",
    "    lv = len(value)\n",
    "    return tuple(int(value[i:i + lv // 3], 16) for i in range(0, lv, lv // 3))\n",
    "\n",
    "\n",
    "def rgb_to_dec(value):\n",
    "    '''\n",
    "    Converts rgb to decimal colours (i.e. divides each value by 256)\n",
    "    value: list (length 3) of RGB values\n",
    "    Returns: list (length 3) of decimal values'''\n",
    "    return [v/256 for v in value]\n",
    "\n",
    "def get_continuous_cmap(hex_list, float_list=None):\n",
    "    ''' creates and returns a color map that can be used in heat map figures.\n",
    "        If float_list is not provided, colour map graduates linearly between each color in hex_list.\n",
    "        If float_list is provided, each color in hex_list is mapped to the respective location in float_list. \n",
    "        \n",
    "        Source\n",
    "        ----------\n",
    "        https://towardsdatascience.com/beautiful-custom-colormaps-with-matplotlib-5bab3d1f0e72\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        hex_list: list of hex code strings\n",
    "        float_list: list of floats between 0 and 1, same length as hex_list. Must start with 0 and end with 1.\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        colour map'''\n",
    "    \n",
    "    rgb_list = [rgb_to_dec(hex_to_rgb(i)) for i in hex_list]\n",
    "    if float_list:\n",
    "        pass\n",
    "    else:\n",
    "        float_list = list(np.linspace(0,1,len(rgb_list)))\n",
    "        \n",
    "    cdict = dict()\n",
    "    for num, col in enumerate(['red', 'green', 'blue']):\n",
    "        col_list = [[float_list[i], rgb_list[i][num], rgb_list[i][num]] for i in range(len(float_list))]\n",
    "        cdict[col] = col_list\n",
    "    cmp = mcolors.LinearSegmentedColormap('my_cmp', segmentdata=cdict, N=256)\n",
    "    return cmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='func_plots'></a>\n",
    "## Functions to be used for data plotting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def setting_map(prcp_index, season_dataset, graph_title, season, legend_text, row_num, col_num, vmin, vmax):\n",
    "     \"\"\"\n",
    "    Returns mapplot lib figure\n",
    "    :prcp_index: string\n",
    "    :season_dataset: string\n",
    "    :graph_title: string\n",
    "    :season: string\n",
    "    :legend_text: string\n",
    "    :row_num: integer\n",
    "    :col_num: integer\n",
    "    :vmin: integer\n",
    "    :vmax: integer\n",
    "    :return: mapplotlib figure\n",
    "    \"\"\"\n",
    "    mm = Basemap(resolution='i',projection='merc',ellps='WGS84',llcrnrlat=49,urcrnrlat=61,llcrnrlon=-9,urcrnrlon=3,lat_ts=20, ax=axs[row_num,col_num])\n",
    "    lons = season_dataset.variables['lon'][:]\n",
    "    lats = season_dataset.variables['lat'][:]\n",
    "    ext_ind = season_dataset.variables[prcp_index][:]\n",
    "    lon, lat = np.meshgrid(lons, lats)\n",
    "    xi, yi = mm(lon, lat)\n",
    "    hex_list = ['#f6eff7', '#c0e0d4', '#8bcdc3', '#4cb9c3', '#3d9ebe', '#3883b6', '#3369ac', '#2d4ea0', '#253494']\n",
    "\n",
    "    cs = mm.pcolor(xi,yi,np.squeeze(ext_ind ),shading='auto', vmin=vmin, vmax=vmax, cmap=get_continuous_cmap(hex_list))\n",
    "    fig.colorbar(cs, ax=axs[row_num,col_num], shrink=0.8, pad=0.05, label=legend_text, orientation = 'horizontal')\n",
    "\n",
    "    # add shp file as coastline\n",
    "    mm.readshapefile('/mnt/d/MRes_dataset/active_data/101_admin/uk_admin_boundary_py_nasa_pp_countryOutlineFromGiovanni', 'uk_boundary')\n",
    "\n",
    "    # draw parallels and meridians.\n",
    "    # Mercator\n",
    "    mm.drawparallels(np.arange(-40,61.,2.),labels=[True, False, False, True])\n",
    "    mm.drawmeridians(np.arange(-20.,21.,2.),labels=[True, False, False, True])\n",
    "    \n",
    "    # set title\n",
    "    # setting legend in bar\n",
    "    if prcp_index in ['r10mm', 'r20mm','r95p']:\n",
    "        title_text = prcp_index.capitalize()\n",
    "    if prcp_index in ['cdd', 'cwd', 'sdii']:\n",
    "        title_text = prcp_index.upper()\n",
    "    if prcp_index in ['rx1day', 'rx5day']:\n",
    "        title_text = prcp_index[0:2].upper() + prcp_index[2:]\n",
    "    if prcp_index == 'prcptot':\n",
    "        title_text = prcp_index[0:4].upper() + prcp_index[4:]\n",
    "        \n",
    "    graph_title = graph_title + '\\n' + season + ' - ' + title_text\n",
    "    axs[row_num,col_num].set_title(graph_title)\n",
    "    \n",
    "    return(mm)\n",
    "\n",
    "def season_mean_UK(season_dataset,lst_seasonNames):\n",
    "    \"\"\"\n",
    "    Returns pandas dataframe contatingn season mean average\n",
    "    :season_dataset: xarray\n",
    "    :lst_seasonNames: list of strings\n",
    "    :return: pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    col_lst = ['r10mm_x', 'r20mm_x', 'cdd_x', 'cwd_x', 'sdii_x', 'rx1day_x', 'rx5day_x', 'prcptot_x', 'r99ptot_x']\n",
    "    col_lst_ord = ['season', 'r10mm', 'r20mm', 'cdd', 'cwd', 'sdii', 'rx1day', 'rx5day', 'prcptot', 'r99ptot']\n",
    "\n",
    "    # Add date as string to help with inner join\n",
    "    arr_averageSeasonOBS = HAD_dataset_season.groupby('time.season').mean(dim='time')\n",
    "    df_averageSeasonOBS = arr_averageSeasonOBS.to_dataframe().reset_index()\n",
    "\n",
    "    arr_averageSeasonOTH = season_dataset.groupby('time.season').mean(dim='time')\n",
    "    df_averageSeasonOTH = arr_averageSeasonOTH.to_dataframe().reset_index()\n",
    "\n",
    "    # Change all timedelta64 (x days) for only the day\n",
    "    indices_lst = ['r10mm', 'r20mm', 'cdd', 'cwd']\n",
    "    for prc_ind in indices_lst:\n",
    "        df_averageSeasonOBS[prc_ind] = np.where(np.isnan(df_averageSeasonOBS[prc_ind] ),np.nan, (df_averageSeasonOBS[prc_ind]  / np.timedelta64(1, 'D')))\n",
    "        df_averageSeasonOTH[prc_ind]  = np.where(np.isnan(df_averageSeasonOTH[prc_ind] ),np.nan, (df_averageSeasonOTH[prc_ind]  / np.timedelta64(1, 'D')))\n",
    "\n",
    "    # Select rows without NaN values for obs dataset (HadUK-Grid) - This make sure we are looking at UK extent\n",
    "    new_df = pd.merge(df_averageSeasonOBS, df_averageSeasonOTH, how='inner', on=['lat','lon','season'])\n",
    "\n",
    "    # Select rows without NaN values - Only looking at the indices of the obs dataset (HadUK-Grid)\n",
    "    # Needed for the statistic analysis. It only remove row if all NaN values\n",
    "    selected_rows = new_df.dropna(subset=col_lst, how='all', axis=0).reset_index()\n",
    "\n",
    "    # Split the joined dataframe into OBS (HAD) and OTH (GPM or ERA)\n",
    "    df_OTH_split = selected_rows.filter(regex='_y')\n",
    "\n",
    "    # create copy of dataframe to avoid the slice error\n",
    "    df_OTH_final = df_OTH_split.copy()\n",
    "\n",
    "    # add lat and lon to split \n",
    "    df_OTH_final['season'] = selected_rows['season']\n",
    "\n",
    "    # rename and reorder columns\n",
    "    df_OTH_final.columns = df_OTH_final.columns.str.replace('_y','')\n",
    "    # re-order\n",
    "    df_OTH_final = df_OTH_final.reindex(columns=col_lst_ord)\n",
    "\n",
    "    # Get average by season for all indices\n",
    "    df_averageSeason_final = df_OTH_final.groupby('season').agg('mean')\n",
    "    return(df_averageSeason_final.reindex(lst_seasonNames).reset_index())\n",
    "\n",
    "def year_mean_UK(season_dataset, dataset_name):\n",
    "    \"\"\"\n",
    "    Returns pandas dataframe contatingn year mean average\n",
    "    :season_dataset: xarray\n",
    "    :lst_seasonNames: list of strings\n",
    "    :return: pandas dataframe\n",
    "    \"\"\"\n",
    "    col_lst = ['r10mm_x', 'r20mm_x', 'cdd_x', 'cwd_x', 'sdii_x', 'rx1day_x', 'rx5day_x', 'prcptot_x', 'r99ptot_x']\n",
    "    col_lst_ord = ['year', 'r10mm', 'r20mm', 'cdd', 'cwd', 'sdii', 'rx1day', 'rx5day', 'prcptot', 'r99ptot']\n",
    "    yrs_lst = [*range(2001,2020,1)]\n",
    "\n",
    "    # Add date as string to help with inner join\n",
    "    if dataset_name != 'HAD':\n",
    "        arr_averageSeasonOTH = season_dataset.groupby('time.year').sum('time')\n",
    "        df_averageSeasonOTH = arr_averageSeasonOTH.to_dataframe().reset_index()\n",
    "\n",
    "        arr_averageSeasonOBS = HAD_dataset_season.groupby('time.year').sum('time')\n",
    "        df_averageSeasonOBS = arr_averageSeasonOBS.to_dataframe().reset_index()\n",
    "\n",
    "        # Change all timedelta64 (x days) for only the day\n",
    "        indices_lst = ['r10mm', 'r20mm', 'cdd', 'cwd']\n",
    "        for prc_ind in indices_lst:\n",
    "            df_averageSeasonOBS[prc_ind] = np.where(np.isnan(df_averageSeasonOBS[prc_ind] ),np.nan, (df_averageSeasonOBS[prc_ind]  / np.timedelta64(1, 'D')))\n",
    "            df_averageSeasonOTH[prc_ind]  = np.where(np.isnan(df_averageSeasonOTH[prc_ind] ),np.nan, (df_averageSeasonOTH[prc_ind]  / np.timedelta64(1, 'D')))\n",
    "            \n",
    "        # Select rows without NaN values for obs dataset (HadUK-Grid) - This make sure we are looking at UK extent\n",
    "        new_df = pd.merge(df_averageSeasonOBS, df_averageSeasonOTH, how='inner', on=['lat','lon','year'])\n",
    "        new_df\n",
    "        # Select rows without NaN values - Only looking at the indices of the obs dataset (HadUK-Grid)\n",
    "        # Needed for the statistic analysis. It only remove row if all NaN values\n",
    "        selected_rows = new_df.dropna(subset=col_lst, how='all', axis=0).reset_index()\n",
    "\n",
    "        # Split the joined dataframe into OBS (HAD) and OTH (GPM or ERA)\n",
    "        df_OTH_split = selected_rows.filter(regex='_y')\n",
    "        df_OTH_split\n",
    "\n",
    "        # create copy of dataframe to avoid the slice error\n",
    "        df_OTH_final = df_OTH_split.copy()\n",
    "\n",
    "        # add year to split \n",
    "        df_OTH_final['year'] = selected_rows['year']\n",
    "\n",
    "         # rename and reorder columns\n",
    "        df_OTH_final.columns = df_OTH_final.columns.str.replace('_y','')\n",
    "        # re-order\n",
    "        df_OTH_final = df_OTH_final.reindex(columns=col_lst_ord)\n",
    "        # Get average by year for all indices\n",
    "        df_averageYear = df_OTH_final.groupby('year').agg('mean')\n",
    "#         df_averageYear = df_averageYear.replace(np.nan, '', regex=True)\n",
    "        df_averageYear = df_averageYear.iloc[1:]\n",
    "\n",
    "    else:\n",
    "        # Remove zero as this affects the average\n",
    "        arr_averageSeasonOBS = season_dataset.groupby('time.year').sum('time')\n",
    "        df_averageSeasonOBS = arr_averageSeasonOBS.to_dataframe().reset_index()\n",
    "        # Change all timedelta64 (x days) for only the day\n",
    "        indices_lst = ['r10mm', 'r20mm', 'cdd', 'cwd']\n",
    "        for prc_ind in indices_lst:\n",
    "            df_averageSeasonOBS[prc_ind] = np.where(np.isnan(df_averageSeasonOBS[prc_ind] ),np.nan, (df_averageSeasonOBS[prc_ind]  / np.timedelta64(1, 'D')))\n",
    "        \n",
    "        # For HADGrid-UK replace zero for NaN to avoid using zero in the mean value\n",
    "        df_averageSeasonOBS = df_averageSeasonOBS.replace(0, np.NaN)\n",
    "\n",
    "        # Drop unnecesary columns\n",
    "        df_averageSeasonOBS.drop(['lat', 'lon', 'percentiles'], axis=1, inplace=True)\n",
    "        # Get average by year for all indices\n",
    "        df_averageYear = df_averageSeasonOBS.groupby('year').agg('mean')\n",
    "#         df_averageYear = df_averageYear.replace(np.nan, '', regex=True)\n",
    "        df_averageYear = df_averageYear.iloc[1:]\n",
    "\n",
    "    return(df_averageYear.reindex(yrs_lst).reset_index().round(2))\n",
    " \n",
    "def indices_plot(ERAdataframe_seasonAverage, GPMdataframe_seasonAverage, HADdataframe_seasonAverage, prcp_index):\n",
    "    \"\"\"\n",
    "    Returns tupple with list of values\n",
    "    :ERAdataframe_seasonAverage: pandas dataframe\n",
    "    :GPMdataframe_seasonAverage: pandas dataframe\n",
    "    :HADdataframe_seasonAverage: pandas dataframe\n",
    "    :prcp_index: string\n",
    "    :return: pandas dataframe\n",
    "    \"\"\"\n",
    "    #setting the data\n",
    "    era_index = ERAdataframe_seasonAverage[prcp_index].tolist()\n",
    "    gpm_index = GPMdataframe_seasonAverage[prcp_index].tolist()\n",
    "    had_index = HADdataframe_seasonAverage[prcp_index].tolist()\n",
    "    \n",
    "    # This deals with NaN values in 2019\n",
    "    if prcp_index in ['r10mm', 'r20mm', 'cdd', 'cwd']:\n",
    "        era_index = era_index[:-1]\n",
    "        gpm_index = gpm_index[:-1]\n",
    "        had_index = had_index[:-1]\n",
    "        \n",
    "    return(era_index, gpm_index, had_index)\n",
    "\n",
    "def plot_setup(subplot_ref,lst_yrs, y_label, x_label):\n",
    "    \"\"\"\n",
    "    Returns maplotlib figure\n",
    "    :subplot_ref: list of integers\n",
    "    :lst_yrs: list of integers\n",
    "    :y_label: string\n",
    "    :x_label: string\n",
    "    :return: mapplotlib figure\n",
    "    \"\"\"\n",
    "    # Set the tick positions\n",
    "    subplot_ref.set_xticks(lst_yrs)\n",
    "    # Set the tick labels\n",
    "    subplot_ref.set_xticklabels(lst_yrs)\n",
    "    subplot_ref.xaxis.set_tick_params(labelsize='x-large')\n",
    "    subplot_ref.yaxis.set_tick_params(labelsize='x-large')\n",
    "    # Set title and axis\n",
    "    subplot_ref.grid()\n",
    "    subplot_ref.set_ylabel(y_label, fontdict={'fontsize': 18, 'fontweight': 'normal'})\n",
    "    subplot_ref.set_xlabel(x_label, fontdict={'fontsize': 18, 'fontweight': 'normal'})\n",
    "    # Set text\n",
    "    subplot_ref.text(0.1, 0.95, label, horizontalalignment='center', verticalalignment=\"top\",\\\n",
    "                  transform=subplot_ref.transAxes, fontsize='x-large', fontweight='bold',\\\n",
    "                  bbox=dict(facecolor='none', edgecolor='black', boxstyle='round'))\n",
    "    # Set legend\n",
    "    subplot_ref.legend(bbox_to_anchor=(0, 1, 1, 0), loc='lower center', fontsize='x-large', ncol=3)\n",
    "    \n",
    "def saving_image(subplot_ref, fldr_plot, file_name):\n",
    "    \"\"\"\n",
    "    Save image output in folder\n",
    "    :subplot_ref: list of integers\n",
    "    :fldr_plot: pathlib folder path\n",
    "    :file_name: string\n",
    "    \"\"\"\n",
    "    extent = subplot_ref.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "    fig.savefig((Path(fldr_plot / file_name)), bbox_inches=extent)\n",
    "#     # Pad the saved area by 10% in the x-direction and 20% in the y-direction, \n",
    "    fig.savefig((Path(fldr_plot / file_name)), bbox_inches=extent.expanded(plot_dmn[0], plot_dmn[1]), dpi=150)\n",
    "    \n",
    "print(sep)\n",
    "print('Mapping functions set')\n",
    "print(sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating maps of statistical outputs\n",
    "\n",
    "This process creates the maps for each statistical result. Only one type of input can be entered here i.e. the results of Pearson's coefficient for HADgridUK to GPM-IMERG for all the seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices_lst = ['r10mm', 'r20mm', 'cdd', 'cwd', 'sdii', 'rx1day', 'rx5day', 'prcptot', 'r95ptot']\n",
    "seasons_lst = ['DJF', 'MAM', 'JJA', 'SON']\n",
    "# title and legend - needs to match the dataset plotted\n",
    "title = 'Levene (p) value\\n HADUK-Grid - ERA5'\n",
    "legend = 'Levene (p)'\n",
    "fileName = 'levene_HADUK-ERA_'\n",
    "fldr_images = Path('/mnt/c/Users/C0060017/Documents/Taught_Material/MRes_Dissertation/Dissertation/Images/levene_ERA')\n",
    "\n",
    "\n",
    "# # Restricting view to the UK - ONLY DO THIS FOR ERA AND GPM-IMERG\n",
    "indices_statsUK_DJF = levene_HAD_ERA_DJF\n",
    "indices_statsUK_MAM = levene_HAD_ERA_MAM\n",
    "indices_statsUK_JJA = levene_HAD_ERA_JJA\n",
    "indices_statsUK_SON = levene_HAD_ERA_SON\n",
    "\n",
    "# Gettting Vmin and Vmax for graphical output - this is the colour bar\n",
    "# Using the xarray to find vmin and vmax for the map display\n",
    "# Each max and min is select by index rather than season\n",
    "\n",
    "vars = ['indices_statsUK_DJF','indices_statsUK_MAM',\n",
    "        'indices_statsUK_JJA','indices_statsUK_SON']\n",
    "\n",
    "df_cbar = pd.DataFrame(columns=['DJF_min','DJF_max','MAM_min','MAM_max',\n",
    "                               'JJA_min','JJA_max','SON_min','SON_max'])\n",
    "# This loop uses the name in the vars list and calls the local variable\n",
    "for prcp_ind in indices_lst:\n",
    "    temp_lst=[]\n",
    "    for name in vars:\n",
    "        min_values = locals()[name][prcp_ind].min().values\n",
    "        max_values = locals()[name][prcp_ind].max().values\n",
    "        temp_lst.append(min_values)\n",
    "        temp_lst.append(max_values)\n",
    "    # add values to dataframe\n",
    "    df_cbar.loc[len(df_cbar)] = temp_lst  \n",
    "\n",
    "# Create final dataframe\n",
    "cbarFinal = pd.DataFrame(columns=['prcp_ind_cbar','vmin','vmax'])\n",
    "# Add data to final dataframe\n",
    "cbarFinal['prcp_ind_cbar'] = indices_lst\n",
    "cbarFinal['vmin'] = df_cbar.min(axis=1)\n",
    "cbarFinal['vmax'] = df_cbar.max(axis=1)\n",
    "# End of Vmin and Vmax for graphical output - this is the colour bar\n",
    "\n",
    "\n",
    "# Start Plotting\n",
    "# set number of rows for subplots\n",
    "n_rows = len(indices_lst)\n",
    "\n",
    "# start subplots\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "fig, axs = plt.subplots(n_rows, 4,figsize=(30,100))\n",
    "\n",
    "# Create the plots\n",
    "i = 0\n",
    "while i < len(indices_lst):\n",
    "    j = 0\n",
    "    for season in seasons_lst:\n",
    "        if season == 'DJF':\n",
    "            prcp_ind = indices_lst[i]\n",
    "            vmin = cbarFinal.loc[cbarFinal.prcp_ind_cbar == prcp_ind,'vmin'].values[0]\n",
    "            vmax = cbarFinal.loc[cbarFinal.prcp_ind_cbar == prcp_ind,'vmax'].values[0]\n",
    "            setting_map(prcp_ind, indices_statsUK_DJF, title, 'DJF', legend, i, j, vmin, vmax)\n",
    "            # saving the subplot\n",
    "            file_name = fileName + season + '_' + prcp_ind + '.png'\n",
    "            saving_image(axs[i, j], fldr_images, file_name,[0.85,1.38])\n",
    "        if season == 'MAM':\n",
    "            prcp_ind = indices_lst[i]\n",
    "            vmin = cbarFinal.loc[cbarFinal.prcp_ind_cbar == prcp_ind,'vmin'].values[0]\n",
    "            vmax = cbarFinal.loc[cbarFinal.prcp_ind_cbar == prcp_ind,'vmax'].values[0]\n",
    "            setting_map(prcp_ind, indices_statsUK_MAM, title, 'MAM', legend, i, j+1, vmin, vmax)\n",
    "            # saving the subplot\n",
    "            file_name = fileName + season + '_' + prcp_ind + '.png'\n",
    "            saving_image(axs[i, j+1], fldr_images, file_name,[0.85,1.38])\n",
    "        if season == 'JJA':\n",
    "            prcp_ind = indices_lst[i]\n",
    "            vmin = cbarFinal.loc[cbarFinal.prcp_ind_cbar == prcp_ind,'vmin'].values[0]\n",
    "            vmax = cbarFinal.loc[cbarFinal.prcp_ind_cbar == prcp_ind,'vmax'].values[0]\n",
    "            setting_map(prcp_ind, indices_statsUK_JJA, title, 'JJA', legend, i, j+2, vmin, vmax)\n",
    "            # saving the subplot\n",
    "            file_name = fileName + season + '_' + prcp_ind + '.png'\n",
    "            saving_image(axs[i, j+2], fldr_images, file_name,[0.85,1.38])\n",
    "            \n",
    "        if season == 'SON':\n",
    "            prcp_ind = indices_lst[i]\n",
    "            vmin = cbarFinal.loc[cbarFinal.prcp_ind_cbar == prcp_ind,'vmin'].values[0]\n",
    "            vmax = cbarFinal.loc[cbarFinal.prcp_ind_cbar == prcp_ind,'vmax'].values[0]\n",
    "            setting_map(prcp_ind, indices_statsUK_SON, title, 'SON', legend, i, j+3, vmin, vmax)\n",
    "            # saving the subplot\n",
    "            file_name = fileName + season + '_' + prcp_ind + '.png'\n",
    "            saving_image(axs[i, j+3], fldr_images, file_name,[0.85,1.38])\n",
    "    i +=1\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Make sure it show a nice layout avoiding overlapping\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating maps of indices average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices_lst = ['r10mm', 'r20mm', 'cdd', 'cwd', 'sdii', 'rx1day', 'rx5day', 'prcptot', 'r95ptot']\n",
    "seasons_lst = ['DJF', 'MAM', 'JJA', 'SON']\n",
    "# title and legend - needs to match the dataset plotted\n",
    "title = 'Mean value ERA5'\n",
    "fileName = 'IndicesMeanAvg_ERA_'\n",
    "DATASET = 'ERA'\n",
    "fldr_images = Path('/mnt/c/Users/C0060017/Documents/Taught_Material/MRes_Dissertation/Dissertation/Images/indicesMeanAvg_ERA')\n",
    "    \n",
    "if DATASET == 'ERA':\n",
    "    indices_avgUK_DJF = IndAvg_ERA_DJF\n",
    "    indices_avgUK_MAM = IndAvg_ERA_MAM\n",
    "    indices_avgUK_JJA = IndAvg_ERA_JJA\n",
    "    indices_avgUK_SON = IndAvg_ERA_SON\n",
    "if DATASET == 'GPM':\n",
    "    indices_avgUK_DJF = IndAvg_GPM_DJF\n",
    "    indices_avgUK_MAM = IndAvg_GPM_MAM\n",
    "    indices_avgUK_JJA = IndAvg_GPM_JJA\n",
    "    indices_avgUK_SON = IndAvg_GPM_SON\n",
    "if DATASET == 'HAD':  \n",
    "    indices_avgUK_DJF = IndAvg_HAD_DJF\n",
    "    indices_avgUK_MAM = IndAvg_HAD_MAM\n",
    "    indices_avgUK_JJA = IndAvg_HAD_JJA\n",
    "    indices_avgUK_SON = IndAvg_HAD_SON\n",
    "\n",
    "# Gettting Vmin and Vmax for graphical output - this is the colour bar\n",
    "# Using the xarray to find vmin and vmax for the map display\n",
    "# Each max and min is select by index rather than season\n",
    "\n",
    "vars = ['indices_avgUK_DJF','indices_avgUK_MAM',\n",
    "        'indices_avgUK_JJA','indices_avgUK_SON']\n",
    "\n",
    "df_cbar = pd.DataFrame(columns=['DJF_min','DJF_max','MAM_min','MAM_max',\n",
    "                               'JJA_min','JJA_max','SON_min','SON_max'])\n",
    "# This loop uses the name in the vars list and calls the local variable\n",
    "for prcp_ind in indices_lst:\n",
    "    temp_lst=[]\n",
    "    for name in vars:\n",
    "        min_values = locals()[name][prcp_ind].min().values\n",
    "        max_values = locals()[name][prcp_ind].max().values\n",
    "        temp_lst.append(min_values)\n",
    "        temp_lst.append(max_values)\n",
    "    # add values to dataframe\n",
    "    df_cbar.loc[len(df_cbar)] = temp_lst  \n",
    "\n",
    "# Create final dataframe\n",
    "cbarFinal = pd.DataFrame(columns=['prcp_ind_cbar','vmin','vmax'])\n",
    "# Add data to final dataframe\n",
    "cbarFinal['prcp_ind_cbar'] = indices_lst\n",
    "cbarFinal['vmin'] = df_cbar.min(axis=1)\n",
    "cbarFinal['vmax'] = df_cbar.max(axis=1)\n",
    "# End of Vmin and Vmax for graphical output - this is the colour bar\n",
    "\n",
    "\n",
    "# set number of rows for subplots\n",
    "n_rows = len(indices_lst)\n",
    "\n",
    "# start subplots\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "fig, axs = plt.subplots(n_rows, 4, figsize=(30,100))\n",
    "\n",
    "# Create the plots\n",
    "i = 0\n",
    "while i < len(indices_lst):\n",
    "    j = 0\n",
    "    # setting legend in bar\n",
    "    if indices_lst[i] in ['r10mm', 'r20mm']:\n",
    "        legend_text = indices_lst[i].capitalize() + ' (days)' \n",
    "    if indices_lst[i] in ['cdd', 'cwd']:\n",
    "        legend_text = indices_lst[i].upper() + ' (days)'\n",
    "    if indices_lst[i] in ['rx1day', 'rx5day']:\n",
    "        legend_text = indices_lst[i][0:2].upper() + indices_lst[i][2:] + ' (mm)'\n",
    "    if indices_lst[i] == 'prcptot':\n",
    "        legend_text = indices_lst[i].upper() + ' (mm)'\n",
    "    if indices_lst[i] in ['r95ptot', 'r99ptot']:\n",
    "        legend_text = indices_lst[i][:4].capitalize() + indices_lst[i][4:].upper()  + ' (%)'\n",
    "    if indices_lst[i] == 'sdii':\n",
    "        legend_text = indices_lst[i].upper() + ' (mm)'\n",
    "  \n",
    "    #Create plots \n",
    "    for season in seasons_lst:\n",
    "        if season == 'DJF':\n",
    "            prcp_ind = indices_lst[i]\n",
    "            vmin = cbarFinal.loc[cbarFinal.prcp_ind_cbar == prcp_ind,'vmin'].values[0]\n",
    "            vmax = cbarFinal.loc[cbarFinal.prcp_ind_cbar == prcp_ind,'vmax'].values[0]\n",
    "            setting_map(prcp_ind, indices_avgUK_DJF, title, season, legend_text, i, j, vmin, vmax)\n",
    "            # saving the subplot\n",
    "            file_name = fileName + season + '_' + prcp_ind + '.png'\n",
    "            saving_image(axs[i, j], fldr_images, file_name,[0.85,1.38])\n",
    "        if season == 'MAM':\n",
    "            prcp_ind = indices_lst[i]\n",
    "            vmin = cbarFinal.loc[cbarFinal.prcp_ind_cbar == prcp_ind,'vmin'].values[0]\n",
    "            vmax = cbarFinal.loc[cbarFinal.prcp_ind_cbar == prcp_ind,'vmax'].values[0]\n",
    "            setting_map(prcp_ind, indices_avgUK_MAM, title, 'MAM', legend_text, i, j+1, vmin, vmax)\n",
    "            # saving the subplot\n",
    "            file_name = fileName + season + '_' + prcp_ind + '.png'\n",
    "            saving_image(axs[i, j+1], fldr_images, file_name,[0.85,1.38])\n",
    "            \n",
    "        if season == 'JJA':\n",
    "            prcp_ind = indices_lst[i]\n",
    "            vmin = cbarFinal.loc[cbarFinal.prcp_ind_cbar == prcp_ind,'vmin'].values[0]\n",
    "            vmax = cbarFinal.loc[cbarFinal.prcp_ind_cbar == prcp_ind,'vmax'].values[0]\n",
    "            setting_map(prcp_ind, indices_avgUK_JJA, title, 'JJA', legend_text, i, j+2, vmin, vmax)\n",
    "            # saving the subplot\n",
    "            file_name = fileName + season + '_' + prcp_ind + '.png'\n",
    "            saving_image(axs[i, j+2], fldr_images, file_name,[0.85,1.38])\n",
    "            \n",
    "        if season == 'SON':\n",
    "            prcp_ind = indices_lst[i]\n",
    "            vmin = cbarFinal.loc[cbarFinal.prcp_ind_cbar == prcp_ind,'vmin'].values[0]\n",
    "            vmax = cbarFinal.loc[cbarFinal.prcp_ind_cbar == prcp_ind,'vmax'].values[0]\n",
    "            setting_map(prcp_ind, indices_avgUK_SON, title, 'SON', legend_text, i, j+3, vmin, vmax)\n",
    "            # saving the subplot\n",
    "            file_name = fileName + season + '_' + prcp_ind + '.png'\n",
    "            saving_image(axs[i, j+3], fldr_images, file_name,[0.85,1.38])\n",
    "    i +=1\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Make sure it show a nice layout avoiding overlapping\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating maps of difference in indices average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices_lst = ['r10mm', 'r20mm', 'cdd', 'cwd', 'sdii', 'rx1day', 'rx5day', 'prcptot', 'r95ptot']\n",
    "seasons_lst = ['DJF', 'MAM', 'JJA', 'SON']\n",
    "# title and legend - needs to match the dataset plotted\n",
    "title = 'Difference Index mean value\\n HadUK-Grid - IMERG'\n",
    "fileName = 'IndicesMeanAvgDifference_GPM_'\n",
    "DATASET = 'GPM'\n",
    "fldr_images = Path('/mnt/c/Users/C0060017/Documents/Taught_Material/MRes_Dissertation/Dissertation/Images/IndicesDiff_GPM')\n",
    "    \n",
    "if DATASET == 'ERA':\n",
    "    diff_indAvgUK_DJF = IndAvg_HAD_DJF - IndAvg_ERA_DJF\n",
    "    diff_indAvgUK_MAM = IndAvg_HAD_MAM - IndAvg_ERA_MAM\n",
    "    diff_indAvgUK_JJA = IndAvg_HAD_JJA - IndAvg_ERA_JJA\n",
    "    diff_indAvgUK_SON = IndAvg_HAD_SON - IndAvg_ERA_SON\n",
    "if DATASET == 'GPM':\n",
    "    diff_indAvgUK_DJF = IndAvg_HAD_DJF - IndAvg_GPM_DJF\n",
    "    diff_indAvgUK_MAM = IndAvg_HAD_MAM - IndAvg_GPM_MAM\n",
    "    diff_indAvgUK_JJA = IndAvg_HAD_JJA - IndAvg_GPM_JJA\n",
    "    diff_indAvgUK_SON = IndAvg_HAD_SON - IndAvg_GPM_SON\n",
    "    \n",
    "# Gettting Vmin and Vmax for graphical output - this is the colour bar\n",
    "# Using the xarray to find vmin and vmax for the map display\n",
    "# Each max and min is select by index rather than season\n",
    "\n",
    "vars = ['diff_indAvgUK_DJF','diff_indAvgUK_MAM',\n",
    "        'diff_indAvgUK_JJA','diff_indAvgUK_SON']\n",
    "\n",
    "df_cbar = pd.DataFrame(columns=['DJF_min','DJF_max','MAM_min','MAM_max',\n",
    "                               'JJA_min','JJA_max','SON_min','SON_max'])\n",
    "# This loop uses the name in the vars list and calls the local variable\n",
    "for prcp_ind in indices_lst:\n",
    "    temp_lst=[]\n",
    "    for name in vars:\n",
    "        min_values = locals()[name][prcp_ind].min().values\n",
    "        max_values = locals()[name][prcp_ind].max().values\n",
    "        temp_lst.append(min_values)\n",
    "        temp_lst.append(max_values)\n",
    "    # add values to dataframe\n",
    "    df_cbar.loc[len(df_cbar)] = temp_lst  \n",
    "\n",
    "# Create final dataframe\n",
    "cbarFinal = pd.DataFrame(columns=['prcp_ind_cbar','vmin','vmax'])\n",
    "# Add data to final dataframe\n",
    "cbarFinal['prcp_ind_cbar'] = indices_lst\n",
    "cbarFinal['vmin'] = df_cbar.min(axis=1)\n",
    "cbarFinal['vmax'] = df_cbar.max(axis=1)\n",
    "# End of Vmin and Vmax for graphical output - this is the colour bar\n",
    "\n",
    "# set number of rows for subplots\n",
    "n_rows = len(indices_lst)\n",
    "\n",
    "# start subplots\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "fig, axs = plt.subplots(n_rows, 4, figsize=(30,100))\n",
    "\n",
    "# Create the plots\n",
    "i = 0\n",
    "while i < len(indices_lst):\n",
    "    j = 0\n",
    "    # setting legend in bar\n",
    "    if indices_lst[i] in ['r10mm', 'r20mm']:\n",
    "        legend_text = indices_lst[i].capitalize() + ' (days)' \n",
    "    if indices_lst[i] in ['cdd', 'cwd']:\n",
    "        legend_text = indices_lst[i].upper() + ' (days)'\n",
    "    if indices_lst[i] in ['rx1day', 'rx5day']:\n",
    "        legend_text = indices_lst[i][0:2].upper() + indices_lst[i][2:] + ' (mm)'\n",
    "    if indices_lst[i] == 'prcptot':\n",
    "        legend_text = indices_lst[i].upper() + ' (mm)'\n",
    "    if indices_lst[i] in ['r95ptot', 'r99ptot']:\n",
    "        legend_text = indices_lst[i][:4].capitalize() + indices_lst[i][4:].upper() + ' (%)'\n",
    "    if indices_lst[i] == 'sdii':\n",
    "        legend_text = indices_lst[i].upper() + ' (mm)'\n",
    " \n",
    "    #Create plots \n",
    "    for season in seasons_lst:\n",
    "        if season == 'DJF':\n",
    "            prcp_ind = indices_lst[i]\n",
    "            vmin = cbarFinal.loc[cbarFinal.prcp_ind_cbar == prcp_ind,'vmin'].values[0]\n",
    "            vmax = cbarFinal.loc[cbarFinal.prcp_ind_cbar == prcp_ind,'vmax'].values[0]\n",
    "            setting_map(prcp_ind, diff_indAvgUK_DJF, title, season, legend_text, i, j, vmin, vmax)\n",
    "            # saving the subplot\n",
    "            file_name = fileName + season + '_' + prcp_ind + '.png'\n",
    "            saving_image(axs[i, j], fldr_images, file_name,[0.85,1.38])\n",
    "        if season == 'MAM':\n",
    "            prcp_ind = indices_lst[i]\n",
    "            vmin = cbarFinal.loc[cbarFinal.prcp_ind_cbar == prcp_ind,'vmin'].values[0]\n",
    "            vmax = cbarFinal.loc[cbarFinal.prcp_ind_cbar == prcp_ind,'vmax'].values[0]\n",
    "            setting_map(prcp_ind, diff_indAvgUK_MAM, title, 'MAM', legend_text, i, j+1, vmin, vmax)\n",
    "            # saving the subplot\n",
    "            file_name = fileName + season + '_' + prcp_ind + '.png'\n",
    "            saving_image(axs[i, j+1], fldr_images, file_name,[0.85,1.38])\n",
    "            \n",
    "        if season == 'JJA':\n",
    "            prcp_ind = indices_lst[i]\n",
    "            vmin = cbarFinal.loc[cbarFinal.prcp_ind_cbar == prcp_ind,'vmin'].values[0]\n",
    "            vmax = cbarFinal.loc[cbarFinal.prcp_ind_cbar == prcp_ind,'vmax'].values[0]\n",
    "            setting_map(prcp_ind, diff_indAvgUK_JJA, title, 'JJA', legend_text, i, j+2, vmin, vmax)\n",
    "            # saving the subplot\n",
    "            file_name = fileName + season + '_' + prcp_ind + '.png'\n",
    "            saving_image(axs[i, j+2], fldr_images, file_name,[0.85,1.38])\n",
    "            \n",
    "        if season == 'SON':\n",
    "            prcp_ind = indices_lst[i]\n",
    "            vmin = cbarFinal.loc[cbarFinal.prcp_ind_cbar == prcp_ind,'vmin'].values[0]\n",
    "            vmax = cbarFinal.loc[cbarFinal.prcp_ind_cbar == prcp_ind,'vmax'].values[0]\n",
    "            setting_map(prcp_ind, diff_indAvgUK_SON, title, 'SON', legend_text, i, j+3, vmin, vmax)\n",
    "            # saving the subplot\n",
    "            file_name = fileName + season + '_' + prcp_ind + '.png'\n",
    "            saving_image(axs[i, j+3], fldr_images, file_name,[0.85,1.38])\n",
    "    i +=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating average time series (season) for each index for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seasons_lst = ['DJF', 'MAM', 'JJA', 'SON']\n",
    "\n",
    "ERA_averageSeason = season_mean_UK(ERA_dataset_season, seasons_lst)\n",
    "GPM_averageSeason = season_mean_UK(GPM_dataset_season, seasons_lst)\n",
    "HAD_averageSeason = season_mean_UK(HAD_dataset_season, seasons_lst)\n",
    "\n",
    "ERA_averageYear_ind = year_mean_UK(ERA_dataset_season,'ERA')\n",
    "GPM_averageYear_ind = year_mean_UK(GPM_dataset_season,'GPM')\n",
    "HAD_averageYear_ind = year_mean_UK(HAD_dataset_season,'HAD')\n",
    "\n",
    "# ERA_averageSeason.reindex(seasons_lst)\n",
    "ERA_averageYear_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Creating plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices_lst = ['r10mm', 'r20mm', 'cdd', 'cwd', 'sdii', 'rx1day', 'rx5day', 'prcptot', 'r95ptot']\n",
    "lst_yrs = [*range(2001,2020,1)]\n",
    "# seasons_lst = ['DJF', 'MAM', 'JJA', 'SON']\n",
    "# x = [1,2,3,4]\n",
    "fldr_images = Path('/mnt/d/MRes_dataset/Images/avgIndicesSeasonPlots')\n",
    "\n",
    "# start subplots\n",
    "n_rows = len(indices_lst)\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "fig, axs = plt.subplots(5, 2, figsize=(50,50))\n",
    "\n",
    "# Create the plots\n",
    "i = 0\n",
    "k = 0\n",
    "while i < len(indices_lst): \n",
    "    # setting legend in bar\n",
    "    if indices_lst[i] in ['r10mm', 'r20mm']:\n",
    "        lbl_yaxis = indices_lst[i].capitalize() + ' (days)' \n",
    "    if indices_lst[i] in ['cdd', 'cwd']:\n",
    "        lbl_yaxis = indices_lst[i].upper() + ' (days)'\n",
    "    if indices_lst[i] in ['rx1day', 'rx5day']:\n",
    "        lbl_yaxis = indices_lst[i][0:2].upper() + indices_lst[i][2:] + ' (mm)'\n",
    "    if indices_lst[i] == 'prcptot':\n",
    "        lbl_yaxis = indices_lst[i][0:4].upper() + indices_lst[i][4:] + ' (mm)'\n",
    "    if indices_lst[i] == 'r95p':\n",
    "        lbl_yaxis = indices_lst[i].capitalize() + ' (mm)'\n",
    "    if indices_lst[i] == 'sdii':\n",
    "        lbl_yaxis = indices_lst[i].upper() + ' (mm)'\n",
    "    \n",
    "    if indices_lst[i] in ['r10mm', 'r20mm', 'cdd']:\n",
    "        lst_yrs_modified = [*range(2001,2019,1)]\n",
    "        data_graph = indices_plot(ERA_averageYear_ind, GPM_averageYear_ind, HAD_averageYear_ind, indices_lst[i])\n",
    "        axs[i,0].plot(lst_yrs_modified, data_graph[0], label = 'ERA5', marker='D')\n",
    "        axs[i,0].plot(lst_yrs_modified, data_graph[1], label = 'GPM-IMERG', marker='v')\n",
    "        axs[i,0].plot(lst_yrs_modified, data_graph[2], label = 'HadUK-Grid', marker='o')\n",
    "        # creating plot\n",
    "        plot_setup(axs[i,0],lst_yrs_modified, lbl_yaxis, 'Seasons')\n",
    "        # saving subplot\n",
    "        file_name = 'average_seasonSeries_allDatasets_UK_' + indices_lst[i] + '.png'\n",
    "        saving_image(axs[i,0], fldr_images, file_name)\n",
    "    elif indices_lst[i] in ['cwd', 'sdii']:\n",
    "        data_graph = indices_plot(ERA_averageYear_ind, GPM_averageYear_ind, HAD_averageYear_ind, indices_lst[i])\n",
    "        axs[i,0].plot(lst_yrs, data_graph[0], label = 'ERA5', marker='D')\n",
    "        axs[i,0].plot(lst_yrs, data_graph[1], label = 'GPM-IMERG', marker='v')\n",
    "        axs[i,0].plot(lst_yrs, data_graph[2], label = 'HadUK-Grid', marker='o')\n",
    "        # creating plot\n",
    "        plot_setup(axs[i,0],lst_yrs, lbl_yaxis, 'Seasons')\n",
    "        # saving subplot\n",
    "        file_name = 'average_seasonSeries_allDatasets_UK_' + indices_lst[i] + '.png'\n",
    "        saving_image(axs[i,0], fldr_images, file_name)\n",
    "\n",
    "    else:\n",
    "        data_graph = indices_plot(ERA_averageYear_ind, GPM_averageYear_ind, HAD_averageYear_ind, indices_lst[i])\n",
    "        axs[k,1].plot(lst_yrs, data_graph[0], label = 'ERA5', marker='D')\n",
    "        axs[k,1].plot(lst_yrs, data_graph[1], label = 'GPM-IMERG', marker='v')\n",
    "        axs[k,1].plot(lst_yrs, data_graph[2], label = 'HadUK-Grid', marker='o')\n",
    "        # creating plot\n",
    "        plot_setup(axs[k,1],lst_yrs, lbl_yaxis, 'Seasons')\n",
    "        if k <= 3:\n",
    "            pass\n",
    "            # saving subplot\n",
    "            file_name = 'average_seasonSeries_allDatasets_UK_' + indices_lst[i] + '.png'\n",
    "            saving_image(axs[k,1], fldr_images, file_name)\n",
    "        \n",
    "        k += 1\n",
    "\n",
    "    i += 1\n",
    "\n",
    "plt.show()\n",
    "# Make sure it show a nice layout avoiding overlapping\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentages of Grid cells\n",
    "Creating table with insignificant different mean (MW) and variance (Levene) and average correlation coefficient at significant level of 5%, 2001-2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    " <p style=\"color:black\"> <b>NOTE: For the loop creating the Getting the percentage of values where H0 can be accepted for MW and Levene, the datasets need to be changed between IMERG and ERA. This step must be run individually for each of the products i.e. you run MW_HAD_ERA_DJF, MW_HAD_ERA_MAM, MW_HAD_ERA_JJA, MW_HAD_ERA_SON; when it is done you then change to MW_HAD_GPM_DJF, MW_HAD_GPM_MAM, MW_HAD_GPM_JJA, MW_HAD_GPM_SON</b></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_lst = ['r10mm', 'r20mm', 'cdd', 'cwd', 'sdii', 'rx1day', 'rx5day', 'prcptot', 'r95ptot']\n",
    "seasons_lst = ['DJF', 'MAM', 'JJA', 'SON']\n",
    "\n",
    "# Create empty df to store values\n",
    "perc_col = ['season', 'stat_test', 'r10mm', 'r20mm', 'cdd', 'cwd', 'sdii', 'rx1day', 'rx5day', 'prcptot', 'r95ptot']\n",
    "df_perc = pd.DataFrame(columns=perc_col)\n",
    "\n",
    "\n",
    "def perc_calc(df,season_name,test_stat):\n",
    "    lst_prc = []\n",
    "    # append season to list\n",
    "    lst_prc.append(season)\n",
    "    lst_prc.append(test_stat)\n",
    "    # Calculate percentage of values where H0 can be accepted\n",
    "    if test_stat in ['MW', 'Levene']:\n",
    "        for prc_ind in col_lst:\n",
    "            df_pvalues_H0 = pd.DataFrame()\n",
    "            # Create mask with the H0 condition\n",
    "            mask = (df[prc_ind] > 0.05)\n",
    "            df_pvalues_H0 = df[mask]\n",
    "            # Get percentage of values where H0 can be accepted\n",
    "            perc_H0 = ((df_pvalues_H0.shape[0] / df.shape[0]) * 100)\n",
    "            lst_prc.append(perc_H0)\n",
    "\n",
    "        # add data to final dataframe\n",
    "        df_perc.loc[len(df_perc)] = lst_prc\n",
    "    if test_stat in ['Pearson', 'Spearman']:\n",
    "        for prc_ind in col_lst:\n",
    "            lst_prc.append(df[prc_ind].mean())\n",
    "\n",
    "        # add data to final dataframe\n",
    "        df_perc.loc[len(df_perc)] = lst_prc\n",
    "    \n",
    "    return(df_perc)\n",
    "    \n",
    "\n",
    "# Getting the percentage of values where H0 can be accepted for MW and Levene\n",
    "for season in seasons_lst:\n",
    "    if season == 'DJF':\n",
    "        df_MW = MW_HAD_ERA_DJF.to_dataframe().reset_index()\n",
    "        df_selectERA_MW = df_MW.dropna(subset=col_lst, how='all', axis=0).reset_index()\n",
    "        \n",
    "        df_LV = levene_HAD_ERA_DJF.to_dataframe().reset_index()\n",
    "        df_selectERA_LV = df_LV.dropna(subset=col_lst, how='all', axis=0).reset_index()\n",
    "        \n",
    "        df_PR = pearson_HAD_ERA_DJF.to_dataframe().reset_index()\n",
    "        df_selectERA_PR = df_PR.dropna(subset=col_lst, how='all', axis=0).reset_index()\n",
    "        \n",
    "        df_SP = spearman_HAD_ERA_DJF.to_dataframe().reset_index()\n",
    "        df_selectERA_SP = df_SP.dropna(subset=col_lst, how='all', axis=0).reset_index()\n",
    "        \n",
    "        perc_calc(df_selectERA_MW,season,'MW')\n",
    "        perc_calc(df_selectERA_LV,season,'Levene')\n",
    "        perc_calc(df_selectERA_PR,season,'Pearson')\n",
    "        perc_calc(df_selectERA_SP,season,'Spearman')\n",
    "        \n",
    "    if season == 'MAM':\n",
    "        df_MW = MW_HAD_ERA_MAM.to_dataframe().reset_index()\n",
    "        df_selectERA_MW = df_MW.dropna(subset=col_lst, how='all', axis=0).reset_index()\n",
    "        \n",
    "        df_LV = levene_HAD_ERA_MAM.to_dataframe().reset_index()\n",
    "        df_selectERA_LV = df_LV.dropna(subset=col_lst, how='all', axis=0).reset_index()\n",
    "        \n",
    "        df_PR = pearson_HAD_ERA_MAM.to_dataframe().reset_index()\n",
    "        df_selectERA_PR = df_PR.dropna(subset=col_lst, how='all', axis=0).reset_index()\n",
    "        \n",
    "        df_SP = spearman_HAD_ERA_MAM.to_dataframe().reset_index()\n",
    "        df_selectERA_SP = df_SP.dropna(subset=col_lst, how='all', axis=0).reset_index()\n",
    "        \n",
    "        perc_calc(df_selectERA_MW,season,'MW')\n",
    "        perc_calc(df_selectERA_LV,season,'Levene')\n",
    "        perc_calc(df_selectERA_PR,season,'Pearson')\n",
    "        perc_calc(df_selectERA_SP,season,'Spearman')\n",
    "        \n",
    "    if season == 'JJA':\n",
    "        df_MW = MW_HAD_ERA_JJA.to_dataframe().reset_index()\n",
    "        df_selectERA_MW = df_MW.dropna(subset=col_lst, how='all', axis=0).reset_index()\n",
    "        \n",
    "        df_LV = levene_HAD_ERA_JJA.to_dataframe().reset_index()\n",
    "        df_selectERA_LV = df_LV.dropna(subset=col_lst, how='all', axis=0).reset_index()\n",
    "        \n",
    "        df_PR = pearson_HAD_ERA_JJA.to_dataframe().reset_index()\n",
    "        df_selectERA_PR = df_PR.dropna(subset=col_lst, how='all', axis=0).reset_index()\n",
    "        \n",
    "        df_SP = spearman_HAD_ERA_JJA.to_dataframe().reset_index()\n",
    "        df_selectERA_SP = df_SP.dropna(subset=col_lst, how='all', axis=0).reset_index()\n",
    "        \n",
    "        perc_calc(df_selectERA_MW,season,'MW')\n",
    "        perc_calc(df_selectERA_LV,season,'Levene')\n",
    "        perc_calc(df_selectERA_PR,season,'Pearson')\n",
    "        perc_calc(df_selectERA_SP,season,'Spearman')\n",
    "        \n",
    "    if season == 'SON':\n",
    "        df_MW = MW_HAD_ERA_SON.to_dataframe().reset_index()\n",
    "        df_selectERA_MW = df_MW.dropna(subset=col_lst, how='all', axis=0).reset_index()\n",
    "        \n",
    "        df_LV = levene_HAD_ERA_SON.to_dataframe().reset_index()\n",
    "        df_selectERA_LV = df_LV.dropna(subset=col_lst, how='all', axis=0).reset_index()\n",
    "        \n",
    "        df_PR = pearson_HAD_ERA_SON.to_dataframe().reset_index()\n",
    "        df_selectERA_PR = df_PR.dropna(subset=col_lst, how='all', axis=0).reset_index()\n",
    "        \n",
    "        df_SP = spearman_HAD_ERA_SON.to_dataframe().reset_index()\n",
    "        df_selectERA_SP = df_SP.dropna(subset=col_lst, how='all', axis=0).reset_index()\n",
    "        \n",
    "        perc_calc(df_selectERA_MW,season,'MW')\n",
    "        perc_calc(df_selectERA_LV,season,'Levene')\n",
    "        perc_calc(df_selectERA_PR,season,'Pearson')\n",
    "        perc_calc(df_selectERA_SP,season,'Spearman')\n",
    "        \n",
    "final_df = df_perc.round(2)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average indices difference value \n",
    "This stage takes the data from section 2.1.2.6. Precipitation indices average for 2001 to 2019 and it is related to section 3.4 Difference Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the datasets\n",
    "diff_indAvgUK_DJF_ERA = IndAvg_HAD_DJF - IndAvg_ERA_DJF\n",
    "diff_indAvgUK_MAM_ERA = IndAvg_HAD_MAM - IndAvg_ERA_MAM\n",
    "diff_indAvgUK_JJA_ERA = IndAvg_HAD_JJA - IndAvg_ERA_JJA\n",
    "diff_indAvgUK_SON_ERA = IndAvg_HAD_SON - IndAvg_ERA_SON\n",
    "\n",
    "diff_indAvgUK_DJF_GPM = IndAvg_HAD_DJF - IndAvg_GPM_DJF\n",
    "diff_indAvgUK_MAM_GPM = IndAvg_HAD_MAM - IndAvg_GPM_MAM\n",
    "diff_indAvgUK_JJA_GPM = IndAvg_HAD_JJA - IndAvg_GPM_JJA\n",
    "diff_indAvgUK_SON_GPM = IndAvg_HAD_SON - IndAvg_GPM_SON\n",
    "\n",
    "# Create empty df to store values\n",
    "diff_col = ['season', 'dataset', 'r10mm', 'r20mm', 'cdd', 'cwd', 'sdii', 'rx1day', 'rx5day', 'prcptot', 'r95ptot']\n",
    "df_diff = pd.DataFrame(columns=diff_col)\n",
    "\n",
    "\n",
    "# Checking if there is a dataframe with all values as NaN\n",
    "vars = ['diff_indAvgUK_DJF_ERA','diff_indAvgUK_MAM_ERA',\n",
    "        'diff_indAvgUK_JJA_ERA','diff_indAvgUK_SON_ERA',\n",
    "        'diff_indAvgUK_DJF_GPM','diff_indAvgUK_MAM_GPM',\n",
    "        'diff_indAvgUK_JJA_GPM','diff_indAvgUK_SON_GPM']\n",
    "\n",
    "# This loop uses the name in the vars list and calls the local variable\n",
    "for name in vars:\n",
    "    # Set dataframe\n",
    "    df = locals()[name].to_dataframe().reset_index()\n",
    "    df_select = df.dropna(subset=col_lst, how='all', axis=0).reset_index()\n",
    "    lst_prc = []\n",
    "    \n",
    "    # append season and dataset to list\n",
    "    lst_prc.append(name.split('_')[2])\n",
    "    lst_prc.append('HadUk-Grid - ' + name.split('_')[3])\n",
    "    \n",
    "    # Calculate percentage of values where H0 can be accepted\n",
    "    for prc_ind in col_lst:\n",
    "        lst_prc.append(df_select[prc_ind].mean())\n",
    "\n",
    "    # add data to final dataframe\n",
    "    df_diff.loc[len(df_diff)] = lst_prc \n",
    "\n",
    "final_diff = df_diff.round(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
